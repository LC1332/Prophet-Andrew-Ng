{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Prophet-Andrew-Ng/blob/main/prophet-code/prophet-random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmVT9EdX7N4U"
      },
      "source": [
        "# 骆驼先知\n",
        "\n",
        "骆驼先知是 李鲁鲁 受到吴恩达的prompt Engineering的启发\n",
        "\n",
        "制作的一个纪伯伦的《先知》的拓展版本\n",
        "\n",
        "运行这个notebook你需要OpenAI的API Token\n",
        "\n",
        "项目链接 [https://github.com/LC1332/Prophet-Andrew-Ng](https://github.com/LC1332/Prophet-Andrew-Ng)\n",
        "\n",
        "骆驼先知是[Luotuo(骆驼)](https://github.com/LC1332/Luotuo-Chinese-LLM)的子项目之一，后者由李鲁鲁，冷子昂，陈启源发起。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## retrieve版本的说明\n",
        "\n",
        "相比于Random的版本，retrieve会根据你的query_topic\n",
        "\n",
        "使用LuotuoBERT去搜索最接近的文本，然后进行In Context的few shot Learning\n",
        "\n",
        "在colab运行时，我们建议使用GPU来进行运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfYhNtHl0UGv"
      },
      "outputs": [],
      "source": [
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vl_bf8oK0UGv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# 导入第三方库\n",
        "\n",
        "openai.api_key  = 'sk-'\n",
        "# 李鲁鲁 在这里设置你的API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4AEM2gk0_sz",
        "outputId": "ad471a32-ab37-43cb-86c9-87a663ab842a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Prophet-Andrew-Ng'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 224 (delta 13), reused 19 (delta 7), pack-reused 189\u001b[K\n",
            "Receiving objects: 100% (224/224), 666.25 KiB | 15.49 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n"
          ]
        }
      ],
      "source": [
        "#从项目中获取数据\n",
        "!git clone https://github.com/LC1332/Prophet-Andrew-Ng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZUEH1Qe0UGw"
      },
      "source": [
        "## 数据读取\n",
        "\n",
        "读取prompt-data中的文本数据，作为in-context-learning的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaYvbBNu0UGw",
        "outputId": "f85b4817-3b47-4692-a046-f6f6d3e13b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "婚姻 292\n",
            "爱 789\n",
            "自由 781\n",
            "孩子 315\n",
            "劳作 935\n"
          ]
        }
      ],
      "source": [
        "# 如果你使用本地的版本，你的路径应该为\n",
        "# prophet_data_folder = './../prophet-data'\n",
        "\n",
        "# 在这里我们考虑colab的版本\n",
        "prophet_data_folder = '/content/Prophet-Andrew-Ng/prophet-data'\n",
        "\n",
        "import os\n",
        "\n",
        "titles = []\n",
        "title_to_text = {}\n",
        "\n",
        "# scan all txt file in prophet_data_folder\n",
        "for file in os.listdir(prophet_data_folder):\n",
        "    if file.endswith('.txt'):\n",
        "        title_name = file[:-4]\n",
        "        titles.append(title_name)\n",
        "\n",
        "        with open(os.path.join(prophet_data_folder, file), 'r') as f:\n",
        "            title_to_text[title_name] = f.read()\n",
        "\n",
        "# report length of each text\n",
        "for title in titles:\n",
        "    print(title, len(title_to_text[title]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHeMCmhc7J6w"
      },
      "source": [
        "我们将在后续课程中深入探究 OpenAI 提供的 ChatCompletion API 的使用方法，在此处，我们先将它封装成一个函数，你无需知道其内部机理，仅需知道调用该函数输入 Prompt 其将会给出对应的 Completion 即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0gQKsQUT7J6w"
      },
      "outputs": [],
      "source": [
        "# 一个封装 OpenAI 接口的函数，参数为 Prompt，返回对应结果\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    '''\n",
        "    prompt: 对应的提示\n",
        "    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，有内测资格的用户可以选择 gpt-4\n",
        "    '''\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # 模型输出的温度系数，控制输出的随机程度\n",
        "    )\n",
        "    # 调用 OpenAI 的 ChatCompletion 接口\n",
        "    return response.choices[0].message[\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrYnOGYB0UGx"
      },
      "source": [
        "在这里我们先假设一个query的主题 \"时间\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_NJoXPn00UGy"
      },
      "outputs": [],
      "source": [
        "query_topic = \"时间\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k18RVHU0UGy"
      },
      "source": [
        "定义先知和市民的身份，这里是为了方便后面的泛化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y6fWF7rE0UGy"
      },
      "outputs": [],
      "source": [
        "prophet_name = \"先知\"\n",
        "\n",
        "citizen_name = \"市民\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1fFNJV40UGy"
      },
      "source": [
        "形成一个shot的句子输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmlWCxMI0UGy",
        "outputId": "029ff658-ba32-4999-ee8b-6351f0611a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<市民>:```请和我们讨论一下\"衣服\"```\n",
            "\n",
            "<先知>:```你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\n",
            "尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 实现一个Python函数 ensemble_one_shot ，输入为prophet_name, citizen_name, sample_topic，sample_answer， 输出为一个组织好的字符串\n",
        "# 例子输入: 先知, 市民, 衣服， \"你们的衣服遮掩了你们许多的美，却不能遮盖住丑。 \\n 尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\"\n",
        "# 例子输出: \"\"\"\n",
        "# <市民>:请和我们讨论一下\"衣服\"\n",
        "\n",
        "# <先知>: 你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\n",
        "# 尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\n",
        "# \"\"\"\n",
        "def ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer):\n",
        "    # 组织对话文本\n",
        "    dialogue = \"<{}>:```请和我们讨论一下\\\"{}\\\"```\\n\\n<{}>:```{}```\\n\\n\".format(citizen_name, sample_topic, prophet_name, sample_answer)\n",
        "    return dialogue\n",
        "\n",
        "# unit test for ensemble_one_shot\n",
        "prophet_name = \"先知\"\n",
        "citizen_name = \"市民\"\n",
        "sample_topic = \"衣服\"\n",
        "sample_answer = \"你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\\n尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\"\n",
        "\n",
        "dialogue = ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer)\n",
        "print(dialogue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g33B4zm0UGz"
      },
      "source": [
        "下面我们来组织完整的prompt，我们假设已经选取了两个主题作为例子 孩子和爱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGPqtBFd0UGz"
      },
      "outputs": [],
      "source": [
        "selected_sample = [\"孩子\",\"爱\"]\n",
        "\n",
        "def organize_prompt( query_topic, selected_sample ):\n",
        "    prompt = \"\"\"你的任务是以一致的风格回答问题。\\n\\n\"\"\"\n",
        "    \n",
        "    for sample_topic in selected_sample:\n",
        "        # find sample_answer in dictionary\n",
        "        sample_answer = title_to_text[sample_topic]\n",
        "        prompt += ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer)\n",
        "\n",
        "    prompt += \"\"\"<{}>:```请和我们讨论一下\"{}\"```\\n\\n\"\"\".format(citizen_name, query_topic)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# write a unit test for organize_prompt\n",
        "query_topic = \"时间\"\n",
        "selected_sample = [\"孩子\",\"爱\"]\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jywQStAp0UGz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcCYGD30UGz"
      },
      "source": [
        "在第一个版本中，我们selected_sample先使用随机的策略\n",
        "\n",
        "在后面的版本中我准备引入LuotuoBERT 去选取更接近主题的问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeyxIbvw0UGz",
        "outputId": "7ec8acb7-d799-483a-a21d-3709ae7744af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['自由', '孩子']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# def function random_select_title, random pick k sample from titles\n",
        "def random_select_title(titles, k):\n",
        "    return random.sample(titles, k)\n",
        "\n",
        "# unit test for random_select_title\n",
        "selected_sample = random_select_title(titles, 2)\n",
        "print(selected_sample)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 升级randome_select到retrieve topics\n",
        "\n",
        "这里我们开始引入LuotuoBERT，相应的项目见 [骆驼嵌入](https://github.com/LC1332/Luotuo-Text-Embedding)\n",
        "\n",
        "我们需要先安装hugging face的代码库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后从hugging face上，载入对应的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from argparse import Namespace\n",
        "# Import our models. The package will take care of downloading the models automatically\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"silk-road/luotuo-bert\")\n",
        "model_args = Namespace(do_mlm=None, pooler_type=\"cls\", temp=0.05, mlp_only_train=False, init_embeddings_model=None)\n",
        "model = AutoModel.from_pretrained(\"silk-road/luotuo-bert\", trust_remote_code=True, model_args=model_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "编写embedding函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    if len(text) > 512:\n",
        "        text = text[:512]\n",
        "    texts = [text]\n",
        "    # Tokenize the text\n",
        "    tokenized_text = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    # Extract the embeddings\n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
        "\n",
        "    return embeddings[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "存储两个list，embeddings和embed_to_title, 记录title和text到embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "embed_to_title = []\n",
        "\n",
        "for title in titles:\n",
        "    text = title_to_text[title]\n",
        "\n",
        "    # divide text with \\n\\n\n",
        "    divided_texts = text.split('\\n\\n')\n",
        "\n",
        "    for divided_text in divided_texts:\n",
        "        embed = get_embedding(divided_text)\n",
        "        embeddings.append(embed)\n",
        "        embed_to_title.append(title)\n",
        "    \n",
        "    embed_title = get_embedding(title)\n",
        "    embeddings.append( embed )\n",
        "    embed_to_title.append(title)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "定义similarity函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cosine_similarity( embed1 , embed2 ):\n",
        "    return torch.nn.functional.cosine_similarity( embed1, embed2 , dim=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "实现搜索函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_embed = get_embedding(query_topic)\n",
        "\n",
        "# implement retrieve_title function, return top k titles\n",
        "def retrieve_title( query_embed, embeddings, embed_to_title, k ):\n",
        "    # compute cosine similarity between query_embed and embeddings\n",
        "    cosine_similarities = []\n",
        "    for embed in embeddings:\n",
        "        cosine_similarities.append( get_cosine_similarity( query_embed, embed ) )\n",
        "    \n",
        "    # sort cosine similarity\n",
        "    sorted_cosine_similarities = sorted( cosine_similarities, reverse=True )\n",
        "\n",
        "    top_k_index = []\n",
        "    top_k_title = []\n",
        "\n",
        "    for i in range(len(sorted_cosine_similarities)):\n",
        "        current_title = embed_to_title[ cosine_similarities.index( sorted_cosine_similarities[i] ) ]\n",
        "        if current_title not in top_k_title:\n",
        "            top_k_title.append( current_title )\n",
        "            top_k_index.append( cosine_similarities.index( sorted_cosine_similarities[i] ) )\n",
        "\n",
        "        if len(top_k_title) == k:\n",
        "            break\n",
        "    \n",
        "    return top_k_title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBqs8-Aq0UG0"
      },
      "source": [
        "集成所有的代码！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG2WvoSs0UG0",
        "outputId": "9001a010-9771-4da0-f408-f54afa0ddab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<先知>:```时间是一条河流，它不停地流淌，无法逆转。\n",
            "我们无法改变时间的流逝，但我们可以改变我们对时间的看法和利用方式。\n",
            "时间是一种珍贵的资源，我们应该珍惜它，充分利用它来实现我们的目标和梦想。\n",
            "我们不能让时间虚度而过，而应该把握每一刻，让时间成为我们的朋友和盟友。\n",
            "同时，我们也应该学会放下过去，珍惜现在，展望未来。\n",
            "时间是一种无形的财富，它的价值不在于它的长度，而在于我们如何利用它。\n",
            "让我们珍惜时间，让它成为我们生命中最宝贵的财富之一。```\n"
          ]
        }
      ],
      "source": [
        "query_topic = \"时间\"\n",
        "selected_sample = random_select_title(titles, 2)\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77BPRdKH0UG0"
      },
      "source": [
        "当然你也可以指定特定的title进行讨论"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1Pbnmw_0UG0",
        "outputId": "4e328f6e-4c47-4099-f330-d33aeab351ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<先知>:```加班，是一种对自己和他人的负责任的表现。\n",
            "当你们在工作中加班时，你们不仅是在完成自己的任务，也是在为公司、为社会做出贡献。\n",
            "加班并不是一种苦役，而是一种自我提升的机会。通过加班，你们可以学习新的技能，提高自己的工作能力，为自己的职业生涯打下坚实的基础。\n",
            "当然，加班也需要适度，不能过度劳累，影响身体健康和家庭生活。\n",
            "在加班时，你们需要保持积极的心态，不要抱怨和消极情绪，要以乐观的态度面对工作和生活。\n",
            "总之，加班是一种责任和机会，需要合理安排和正确对待。```\n"
          ]
        }
      ],
      "source": [
        "query_topic = \"加班\"\n",
        "selected_sample = [\"劳作\",\"自由\"]\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQuza6h2PX1"
      },
      "source": [
        "后续\n",
        "\n",
        "+ 增加更好的前置提示词（提前总结先知的文风）\n",
        "\n",
        "+ 补充完整的prophet data（一共20+个，现在只有5个）\n",
        "\n",
        "+ 使用luotuoBERT索引更相关的主题\n",
        "\n",
        "+ 做一个gradio的前端"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUoTSfo92fRL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
