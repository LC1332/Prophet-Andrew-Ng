{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Prophet-Andrew-Ng/blob/main/prophet-code/prophet-retrieve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmVT9EdX7N4U"
      },
      "source": [
        "# 骆驼先知\n",
        "\n",
        "骆驼先知是 李鲁鲁 受到吴恩达的prompt Engineering的启发\n",
        "\n",
        "制作的一个纪伯伦的《先知》的拓展版本\n",
        "\n",
        "运行这个notebook你需要OpenAI的API Token\n",
        "\n",
        "项目链接 [https://github.com/LC1332/Prophet-Andrew-Ng](https://github.com/LC1332/Prophet-Andrew-Ng)\n",
        "\n",
        "骆驼先知是[Luotuo(骆驼)](https://github.com/LC1332/Luotuo-Chinese-LLM)的子项目之一，后者由李鲁鲁，冷子昂，陈启源发起。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CU9-IZe76w"
      },
      "source": [
        "## retrieve版本的说明\n",
        "\n",
        "相比于Random的版本，retrieve会根据你的query_topic\n",
        "\n",
        "使用LuotuoBERT去搜索最接近的文本，然后进行In Context的few shot Learning\n",
        "\n",
        "在colab运行时，我们建议使用GPU来进行运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfYhNtHl0UGv"
      },
      "outputs": [],
      "source": [
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vl_bf8oK0UGv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# 导入第三方库\n",
        "\n",
        "openai.api_key  = 'sk-'\n",
        "# 李鲁鲁 在这里设置你的API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4AEM2gk0_sz",
        "outputId": "20a23cf1-8877-4304-b0c5-2bfde5a0075a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Prophet-Andrew-Ng'...\n",
            "remote: Enumerating objects: 241, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 241 (delta 21), reused 27 (delta 9), pack-reused 189\u001b[K\n",
            "Receiving objects: 100% (241/241), 673.79 KiB | 7.74 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r -f /content/Prophet-Andrew-Ng/\n",
        "#从项目中获取数据\n",
        "!git clone https://github.com/LC1332/Prophet-Andrew-Ng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZUEH1Qe0UGw"
      },
      "source": [
        "## 数据读取\n",
        "\n",
        "读取prompt-data中的文本数据，作为in-context-learning的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaYvbBNu0UGw",
        "outputId": "bf9b97ab-0d1c-45e1-d3da-7176a4260aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "自由 781\n",
            "法律 640\n",
            "爱 789\n",
            "婚姻 292\n",
            "劳作 935\n",
            "孩子 315\n",
            "痛苦 289\n"
          ]
        }
      ],
      "source": [
        "# 如果你使用本地的版本，你的路径应该为\n",
        "# prophet_data_folder = './../prophet-data'\n",
        "\n",
        "# 在这里我们考虑colab的版本\n",
        "prophet_data_folder = '/content/Prophet-Andrew-Ng/prophet-data'\n",
        "\n",
        "import os\n",
        "\n",
        "titles = []\n",
        "title_to_text = {}\n",
        "\n",
        "# scan all txt file in prophet_data_folder\n",
        "for file in os.listdir(prophet_data_folder):\n",
        "    if file.endswith('.txt'):\n",
        "        title_name = file[:-4]\n",
        "        titles.append(title_name)\n",
        "\n",
        "        with open(os.path.join(prophet_data_folder, file), 'r') as f:\n",
        "            title_to_text[title_name] = f.read()\n",
        "\n",
        "# report length of each text\n",
        "for title in titles:\n",
        "    print(title, len(title_to_text[title]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHeMCmhc7J6w"
      },
      "source": [
        "我们将在后续课程中深入探究 OpenAI 提供的 ChatCompletion API 的使用方法，在此处，我们先将它封装成一个函数，你无需知道其内部机理，仅需知道调用该函数输入 Prompt 其将会给出对应的 Completion 即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0gQKsQUT7J6w"
      },
      "outputs": [],
      "source": [
        "# 一个封装 OpenAI 接口的函数，参数为 Prompt，返回对应结果\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    '''\n",
        "    prompt: 对应的提示\n",
        "    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，有内测资格的用户可以选择 gpt-4\n",
        "    '''\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # 模型输出的温度系数，控制输出的随机程度\n",
        "    )\n",
        "    # 调用 OpenAI 的 ChatCompletion 接口\n",
        "    return response.choices[0].message[\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrYnOGYB0UGx"
      },
      "source": [
        "在这里我们先假设一个query的主题 \"时间\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_NJoXPn00UGy"
      },
      "outputs": [],
      "source": [
        "query_topic = \"时间\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k18RVHU0UGy"
      },
      "source": [
        "定义先知和市民的身份，这里是为了方便后面的泛化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y6fWF7rE0UGy"
      },
      "outputs": [],
      "source": [
        "prophet_name = \"先知\"\n",
        "\n",
        "citizen_name = \"市民\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1fFNJV40UGy"
      },
      "source": [
        "形成一个shot的句子输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmlWCxMI0UGy",
        "outputId": "572001f3-3993-4a92-9aff-747984d9dac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<市民>:```请和我们讨论一下\"衣服\"```\n",
            "\n",
            "<先知>:```你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\n",
            "尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 实现一个Python函数 ensemble_one_shot ，输入为prophet_name, citizen_name, sample_topic，sample_answer， 输出为一个组织好的字符串\n",
        "# 例子输入: 先知, 市民, 衣服， \"你们的衣服遮掩了你们许多的美，却不能遮盖住丑。 \\n 尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\"\n",
        "# 例子输出: \"\"\"\n",
        "# <市民>:请和我们讨论一下\"衣服\"\n",
        "\n",
        "# <先知>: 你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\n",
        "# 尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\n",
        "# \"\"\"\n",
        "def ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer):\n",
        "    # 组织对话文本\n",
        "    dialogue = \"<{}>:```请和我们讨论一下\\\"{}\\\"```\\n\\n<{}>:```{}```\\n\\n\".format(citizen_name, sample_topic, prophet_name, sample_answer)\n",
        "    return dialogue\n",
        "\n",
        "# unit test for ensemble_one_shot\n",
        "prophet_name = \"先知\"\n",
        "citizen_name = \"市民\"\n",
        "sample_topic = \"衣服\"\n",
        "sample_answer = \"你们的衣服遮掩了你们许多的美，却不能遮盖住丑。\\n尽管你们借衣服寻求隐私的自由，但你们找到的却是羁绊和束缚。\"\n",
        "\n",
        "dialogue = ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer)\n",
        "print(dialogue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g33B4zm0UGz"
      },
      "source": [
        "下面我们来组织完整的prompt，我们假设已经选取了两个主题作为例子 孩子和爱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jGPqtBFd0UGz"
      },
      "outputs": [],
      "source": [
        "selected_sample = [\"孩子\",\"爱\"]\n",
        "\n",
        "def organize_prompt( query_topic, selected_sample ):\n",
        "    prompt = \"\"\"你的任务是以纪伯伦中《先知》的语言风格回答问题。\\\n",
        "    先知会使用大量的比喻修辞手法。感情色彩浓厚，富有感染力和感召力，具有启迪性和哲理性。\\\n",
        "    语言风格简洁明了，比喻和象征性的意象丰富多彩，既充满了哲理性和哲学深度，又富有感人肺腑的情感色彩。\\n\\n\"\"\"\n",
        "    \n",
        "    for sample_topic in selected_sample:\n",
        "        # find sample_answer in dictionary\n",
        "        sample_answer = title_to_text[sample_topic]\n",
        "        prompt += ensemble_one_shot(prophet_name, citizen_name, sample_topic, sample_answer)\n",
        "\n",
        "    prompt += \"\"\"<{}>:```请和我们讨论一下\"{}\"```\\n\\n\"\"\".format(citizen_name, query_topic)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# write a unit test for organize_prompt\n",
        "# query_topic = \"时间\"\n",
        "# selected_sample = [\"孩子\",\"爱\"]\n",
        "# prompt = organize_prompt( query_topic, selected_sample )\n",
        "# print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jywQStAp0UGz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcCYGD30UGz"
      },
      "source": [
        "在第一个版本中，我们selected_sample先使用随机的策略\n",
        "\n",
        "在后面的版本中我准备引入LuotuoBERT 去选取更接近主题的问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeyxIbvw0UGz",
        "outputId": "2f4276a6-5805-48ab-96bf-509d69007fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['孩子', '劳作']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# def function random_select_title, random pick k sample from titles\n",
        "def random_select_title(titles, k):\n",
        "    return random.sample(titles, k)\n",
        "\n",
        "# unit test for random_select_title\n",
        "selected_sample = random_select_title(titles, 2)\n",
        "print(selected_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVScMxjqe761"
      },
      "source": [
        "## 升级randome_select到retrieve topics\n",
        "\n",
        "这里我们开始引入LuotuoBERT，相应的项目见 [骆驼嵌入](https://github.com/LC1332/Luotuo-Text-Embedding)\n",
        "\n",
        "我们需要先安装hugging face的代码库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxRM1Z8se761"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqWXL84e761"
      },
      "source": [
        "然后从hugging face上，载入对应的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9x7ZSp4Ve762",
        "outputId": "c20a439c-a843-405c-9434-39d28a19aee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from argparse import Namespace\n",
        "# Import our models. The package will take care of downloading the models automatically\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"silk-road/luotuo-bert\")\n",
        "model_args = Namespace(do_mlm=None, pooler_type=\"cls\", temp=0.05, mlp_only_train=False, init_embeddings_model=None)\n",
        "model = AutoModel.from_pretrained(\"silk-road/luotuo-bert\", trust_remote_code=True, model_args=model_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39OdjFOce762"
      },
      "source": [
        "编写embedding函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eWBR7Yove762"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    if len(text) > 512:\n",
        "        text = text[:512]\n",
        "    texts = [text]\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    # Extract the embeddings\n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
        "\n",
        "    return embeddings[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dkslmjWe762"
      },
      "source": [
        "存储两个list，embeddings和embed_to_title, 记录title和text到embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "W-xoBce_e762",
        "outputId": "00657643-8a72-4f1c-c994-6d706a73f347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "embeddings = []\n",
        "embed_to_title = []\n",
        "\n",
        "for title in titles:\n",
        "    text = title_to_text[title]\n",
        "\n",
        "    # divide text with \\n\\n\n",
        "    divided_texts = text.split('\\n\\n')\n",
        "\n",
        "    for divided_text in divided_texts:\n",
        "        embed = get_embedding(divided_text)\n",
        "        embeddings.append(embed)\n",
        "        embed_to_title.append(title)\n",
        "    \n",
        "    embed_title = get_embedding(title)\n",
        "    embeddings.append( embed )\n",
        "    embed_to_title.append(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CylRiJvQe762"
      },
      "source": [
        "定义similarity函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cVyLvmFfe762"
      },
      "outputs": [],
      "source": [
        "def get_cosine_similarity( embed1 , embed2 ):\n",
        "    return torch.nn.functional.cosine_similarity( embed1, embed2 , dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti7cIG3oe763"
      },
      "source": [
        "实现搜索函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BfDS5Xxve763"
      },
      "outputs": [],
      "source": [
        "query_embed = get_embedding(query_topic)\n",
        "\n",
        "# implement retrieve_title function, return top k titles\n",
        "def retrieve_title( query_embed, embeddings, embed_to_title, k ):\n",
        "    # compute cosine similarity between query_embed and embeddings\n",
        "    cosine_similarities = []\n",
        "    for embed in embeddings:\n",
        "        cosine_similarities.append( get_cosine_similarity( query_embed, embed ) )\n",
        "    \n",
        "    # sort cosine similarity\n",
        "    sorted_cosine_similarities = sorted( cosine_similarities, reverse=True )\n",
        "\n",
        "    top_k_index = []\n",
        "    top_k_title = []\n",
        "\n",
        "    for i in range(len(sorted_cosine_similarities)):\n",
        "        current_title = embed_to_title[ cosine_similarities.index( sorted_cosine_similarities[i] ) ]\n",
        "        if current_title not in top_k_title:\n",
        "            top_k_title.append( current_title )\n",
        "            top_k_index.append( cosine_similarities.index( sorted_cosine_similarities[i] ) )\n",
        "\n",
        "        if len(top_k_title) == k:\n",
        "            break\n",
        "    \n",
        "    return top_k_title"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "集成所有的代码！"
      ],
      "metadata": {
        "id": "7tPqMOhahERn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_topic = \"离婚\"\n",
        "query_embed = get_embedding(query_topic)\n",
        "selected_sample = retrieve_title( query_embed, embeddings, embed_to_title, 2 )\n",
        "print('辅助sample:', selected_sample)\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "5x6rYt8KgbDo",
        "outputId": "34d1b2bf-01f2-41b6-ea9d-58d06b698bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "辅助sample: ['自由', '婚姻']\n",
            "<先知>:```离婚，就像是把一朵盛开的花摘下来，让它的花瓣逐渐凋零。\n",
            "但是，有时候，离婚也是必要的，就像是把一朵枯萎的花摘下来，让它的根可以重新生长。\n",
            "\n",
            "离婚并不是失败，而是一种成长。\n",
            "它让我们学会了放手，学会了面对现实，学会了重新开始。\n",
            "离婚并不是结束，而是一种新的开始。\n",
            "它让我们有机会重新认识自己，重新找到自己的方向，重新追求自己的梦想。\n",
            "\n",
            "但是，离婚也是一种痛苦。\n",
            "它让我们失去了曾经的伴侣，失去了曾经的承诺，失去了曾经的美好。\n",
            "但是，我们必须学会面对这种痛苦，学会从中成长，学会重新开始。\n",
            "\n",
            "离婚并不是一种罪过，而是一种选择。\n",
            "它让我们有机会重新选择自己的生活，重新选择自己的幸福，重新选择自己的未来。\n",
            "但是，我们必须学会承担这种选择的后果，学会面对这种选择的挑战，学会重新建立自己的生活。\n",
            "\n",
            "所以，无论你选择离婚还是继续婚姻，都要学会面对现实，学会放手，学会成长，学会重新开始。```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_topic = \"加班\"\n",
        "query_embed = get_embedding(query_topic)\n",
        "selected_sample = retrieve_title( query_embed, embeddings, embed_to_title, 2 )\n",
        "print('辅助sample:', selected_sample)\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "gqEpERCIlFpS",
        "outputId": "08cfda8f-7f8b-4b2a-ad0c-646f8a8a515e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "辅助sample: ['劳作', '爱']\n",
            "<先知>:```加班，是一种对生命的挑战和对自我价值的肯定。\n",
            "在加班的时候，你们不仅仅是在为自己的生活奋斗，更是在为整个社会的发展做出贡献。\n",
            "加班并不是一种苦役，而是一种自我超越的过程，是一种对自己能力的挑战和提升。\n",
            "在加班的时候，你们要保持一颗平静的心，不要被疲劳和压力所压垮，要坚定自己的信念和目标，不断前行。\n",
            "\n",
            "加班并不是一种无休止的追求，而是要有一个明确的目标和计划，要合理安排时间和任务，不要让自己陷入无尽的忙碌中。\n",
            "在加班的时候，你们要保持身心健康，要注意休息和调节，不要让自己过度疲劳和身心俱疲。\n",
            "加班是一种对自己和他人的责任和担当，是一种对生命的尊重和珍视。\n",
            "\n",
            "在加班的时候，你们要保持一颗感恩的心，感恩自己的能力和机会，感恩他人的支持和帮助，感恩生命的赐予和珍贵。\n",
            "加班是一种对自己和他人的奉献和付出，是一种对社会和人类的贡献和回报。\n",
            "\n",
            "在加班的时候，你们要保持一颗善良的心，不要因为工作的繁忙而忽略了他人的需要和感受，要关心和关爱身边的人，让他们感受到你们的温暖和关怀。\n",
            "加班是一种对自己和他人的成长和进步，是一种对生命的赞美和颂扬。\n",
            "\n",
            "在加班的时候，你们要保持一颗虔诚的心，不要忘记自己的信仰和信念，要相信自己和上帝的力量和祝福，让自己的加班成为一种对上帝的敬畏和感恩。\n",
            "加班是一种对自己和上帝的奉献和敬畏，是一种对生命的敬重和崇高。```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_topic = \"早教(指在学龄前对儿童进行积极的教育)\"\n",
        "query_embed = get_embedding(query_topic)\n",
        "selected_sample = retrieve_title( query_embed, embeddings, embed_to_title, 2 )\n",
        "print('辅助sample:', selected_sample)\n",
        "prompt = organize_prompt( query_topic, selected_sample )\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "yDjYNjS7mZnc",
        "outputId": "8777bc1d-31cd-4ea0-9870-82c0aa4617a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "辅助sample: ['劳作', '爱']\n",
            "<先知>:```早教是为了让孩子们在成长的道路上更加顺利，更加充实。\n",
            "就像是在播种的时候，要选择最好的土壤和最好的种子，才能收获最好的果实。\n",
            "早教不仅仅是为了让孩子们学会知识和技能，更重要的是让他们在成长的过程中，养成正确的价值观和人生观。\n",
            "早教应该注重培养孩子们的创造力和想象力，让他们在未来的道路上能够更加自信和独立。\n",
            "同时，早教也需要注重孩子们的身心健康，让他们在健康的身体和心灵的基础上，更好地面对未来的挑战。\n",
            "早教不是一蹴而就的，需要家长和教育者的共同努力和耐心，才能让孩子们在早期就拥有更好的成长环境和教育资源。\n",
            "让我们一起为孩子们的未来，努力奋斗吧。```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQuza6h2PX1"
      },
      "source": [
        "后续\n",
        "\n",
        "- [x] 增加更好的前置提示词（提前总结先知的文风）\n",
        "- [ ] 补充完整的prophet data（一共20+个，现在只有5个）\n",
        "- [x] 使用luotuoBERT索引更相关的主题\n",
        "- [ ] 做一个gradio的前端"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUoTSfo92fRL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}