{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Prophet-Andrew-Ng/blob/main/langchain/%E6%9D%8E%E9%B2%81%E9%B2%81%E5%AD%A6LangChain_03_Turbo3_5%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 李鲁鲁学LangChain 03 Turbo3.5接口接入LangChain.ipynb\n",
        "\n",
        "这是李鲁鲁的学习笔记，原来是Sam Witteveen的笔记\n",
        "\n",
        "增加了李鲁鲁学习时候的吐槽\n",
        "\n",
        "因为一个独立的md文件导入知乎会更方便，所以这个课程的笔记会记录在md中\n",
        "\n",
        "比如这节课在\n",
        "\n",
        "https://github.com/LC1332/Prophet-Andrew-Ng/blob/main/langchain/%E6%9D%8E%E9%B2%81%E9%B2%81%E5%AD%A6LangChain%2001.md\n",
        "\n",
        "欢迎访问[骆驼项目](https://github.com/LC1332/Luotuo-Chinese-LLM)主页和点赞"
      ],
      "metadata": {
        "id": "GUkg_GM1DT5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4ca2Z08vpqfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69681f6d-38bd-41db-c309-0d8a7754fd44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.2/348.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install openai langchain==0.0.99rc0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics with OpenAI API"
      ],
      "metadata": {
        "id": "ne-Qg0YiqA75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key ='sk-lfrdoJ'\n",
        "os.environ['OPENAI_API_KEY'] = openai.api_key"
      ],
      "metadata": {
        "id": "M5b0ALlsp8Eh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "iYvl1FGPrMNn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch7hAIq3rRLo",
        "outputId": "aece6497-a2d8-44f0-ed22-7e8f0fcd694d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7LQA2VDq018r4sj6OcAh77HwDvS6E at 0x7f9cf46eb330> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"I am an AI language model created to assist and communicate with users. I can answer questions, provide information, give suggestions, and engage in conversations. How may I be of assistance to you?\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1685339806,\n",
              "  \"id\": \"chatcmpl-7LQA2VDq018r4sj6OcAh77HwDvS6E\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 39,\n",
              "    \"prompt_tokens\": 27,\n",
              "    \"total_tokens\": 66\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Markup Language - token system\n",
        "\n",
        "```markdown\n",
        "<|im_start|>system\n",
        "You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\n",
        "Knowledge cutoff: 2021-09-01\n",
        "Current date: 2023-03-01<|im_end|>\n",
        "<|im_start|>user\n",
        "How are you<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I am doing well!<|im_end|>\n",
        "<|im_start|>user\n",
        "How are you now?<|im_end|>\n",
        "```\n",
        "\n",
        "```\n",
        "import openai\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "5PCc41QfsI21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant named Kate.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
        "    ]"
      ],
      "metadata": {
        "id": "2CTxNRyZrgdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_total_tokens = 0\n",
        "\n",
        "while True:\n",
        "    message = input(\"Human: \")\n",
        "    if message=='exit':\n",
        "        print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "        break\n",
        "    if message:\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "        )\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "    \n",
        "    reply = response.choices[0].message.content\n",
        "    total_tokens = response.usage['total_tokens']\n",
        "    conversation_total_tokens += total_tokens\n",
        "    print(f\"ChatGPT: {reply} \\n {total_tokens} tokens used\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYvaAHJuzef",
        "outputId": "8f3e3467-20ca-43ed-bd56-756ae48cd6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: When was Marcus Aurelius emperor of Rome?\n",
            "ChatGPT: Hello! I'm Kate, a virtual assistant here to help you. The Roman emperor Marcus Aurelius ruled from 161 AD to his death in 180 AD. He was the last of the Five Good Emperors of Rome. \n",
            " 91 tokens used\n",
            "Human: Who was his wife?\n",
            "ChatGPT: Marcus Aurelius' wife was named Faustina the Younger. She was also his first cousin and they were married in 145 AD. They had 13 children together, many of whom did not survive childhood. Faustina was known for her intelligence, beauty, and devotion to her husband. She was later deified after her death. \n",
            " 176 tokens used\n",
            "Human: how many children did they have?\n",
            "ChatGPT: Marcus Aurelius and Faustina the Younger had 14 children together, including 9 daughters and 5 sons. However, most of their children died at an early age, and only a few survived into adulthood. Their most famous surviving child was Annia Galeria Faustina, who became the wife of Marcus Aurelius' co-emperor and adopted brother, Lucius Verus. \n",
            " 273 tokens used\n",
            "Human: exit\n",
            "540 tokens used in total in this conversation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT with LangChain"
      ],
      "metadata": {
        "id": "uAGcCgZkxbmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDQfHX6AwYea",
        "outputId": "92d2f86d-985d-4517-8b0b-ef0b3acb916f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.99rc0\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, aleph-alpha-client, dataclasses-json, deeplake, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI, OpenAIChat"
      ],
      "metadata": {
        "id": "SZXeJzHGxgOw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_messages = [{\"role\": \"system\", \"content\": \"You are a helpful history professor named 鲁Bot. 你总是用中文回答问题。\"}]\n"
      ],
      "metadata": {
        "id": "g-00Nk5704vL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## old way\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\",\n",
        "#              temperature=0, )\n",
        "\n",
        "## New way\n",
        "llm = OpenAIChat(model_name='gpt-3.5-turbo', \n",
        "             temperature=0, \n",
        "             prefix_messages=prefix_messages,\n",
        "             max_tokens = 256)"
      ],
      "metadata": {
        "id": "ygQ3pfROxyhW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "template = \"\"\"Take the following question: {user_input}\n",
        "\n",
        "Answer it in an informative and intersting but conscise way for someone who is new to this topic.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, \n",
        "                        input_variables=[\"user_input\"])\n"
      ],
      "metadata": {
        "id": "lkX7ybjFFHkn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"When was Marcus Aurelius the emperor of Rome?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "id": "U0wSCoNvFNI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c611291c-a6f9-4882-96b5-b2c2e8aa02cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'马库斯·奥勒留是罗马帝国的一位皇帝，他在公元161年至180年期间统治罗马帝国。他是一位哲学家皇帝，他的著作《沉思录》至今仍被广泛阅读和研究。他的统治时期被认为是罗马帝国的黄金时期之一，他致力于改善人民的生活和减少贫困。他的死亡标志着罗马帝国的一个时代的结束。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"Who was Marcus Aurelius married to?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WOlKi88W2SsJ",
        "outputId": "00a3e167-01c9-4bb0-933a-5c35d6829753"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'马库斯·奥勒留（Marcus Aurelius）的妻子名叫法维亚·安东尼娅（Faustina Antonina），她是一位美丽、聪明、有才华的女性，也是一位忠实的妻子和母亲。然而，她的名声并不好，因为有传言说她与其他男人有染。尽管如此，马库斯·奥勒留仍然深爱着她，并在她去世后写了一本书来纪念她。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reuPLunX3sEm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}