{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Prophet-Andrew-Ng/blob/main/langchain/%E6%9D%8E%E9%B2%81%E9%B2%81%E5%AD%A6LangChain_25_Kor%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 李鲁鲁学LangChain 25 Kor信息抽取\n",
        "\n",
        "这是李鲁鲁的学习笔记，对Sam Witteveen的油管课程的记录\n",
        "\n",
        "增加了李鲁鲁学习时候的吐槽\n",
        "\n",
        "因为一个独立的md文件导入知乎会更方便，所以这个课程的笔记会记录在md中\n",
        "\n",
        "比如这节课在\n",
        "\n",
        "https://github.com/LC1332/Prophet-Andrew-Ng/blob/main/langchain/%E6%9D%8E%E9%B2%81%E9%B2%81%E5%AD%A6LangChain%2025.md\n",
        "\n",
        "欢迎访问[骆驼项目](https://github.com/LC1332/Luotuo-Chinese-LLM)主页和点赞\n",
        "\n",
        "这篇笔记同时也是我们用来抽取Chat凉宫春日台本的原型代码"
      ],
      "metadata": {
        "id": "XgYaoQc_-iiP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVEp_XM7-iFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1yMHANTwLDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://eyurtsev.github.io/kor/nested_objects.html"
      ],
      "metadata": {
        "id": "nDnYqwFcwLiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82b089c-ae78-488a-eb2d-ce4e90d4fb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai google-search-results tiktoken\n",
        "!pip -q install kor markdownify"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a1cb1d-acad-45d3-c8cc-5f3d65d2414c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.245\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: kor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kor.extraction import create_extraction_chain\n",
        "from kor.nodes import Object, Text, Number\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "4g4MiT_ZBiNk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2oZ858-xUjFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "1zM0lwApu8jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = Object(\n",
        "    id=\"personal_info\",\n",
        "    description=\"Personal information about a given person.\",\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"first_name\",\n",
        "            description=\"The first name of the person\",\n",
        "            # examples=[(\"John Smith went to the store\", \"John\")]\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"last_name\",\n",
        "            description=\"The last name of the person\",\n",
        "            # examples=[(\"John Smith went to the store\", \"Smith\")],\n",
        "        ),\n",
        "        Number(\n",
        "            id=\"age\",\n",
        "            description=\"The age of the person in years.\",\n",
        "            # examples=[(\"23 years old\", \"23\"), (\"I turned three on sunday\", \"3\")]\n",
        "        ),\n",
        "    ],\n",
        "    examples=[\n",
        "        (\n",
        "            \"John Smith was 23 years old. He was very tall. He knew Jane Doe. She was 5 years old.\",\n",
        "            [\n",
        "                {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 23},\n",
        "                {\"first_name\": \"Jane\", \"last_name\": \"Doe\", \"age\": 5},\n",
        "            ],\n",
        "        )\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "PkGvJwaRv-zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(llm, schema)\n",
        "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVawvIr42EUI",
        "outputId": "042eafb0-6d37-474d-d5da-355c13f99eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your goal is to extract structured information from the user's input that matches the form described below. When extracting information please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
            "\n",
            "```TypeScript\n",
            "\n",
            "personal_info: Array<{ // Personal information about a given person.\n",
            " first_name: string // The first name of the person\n",
            " last_name: string // The last name of the person\n",
            " age: number // The age of the person in years.\n",
            "}>\n",
            "```\n",
            "\n",
            "\n",
            "Please output the extracted information in CSV format in Excel dialect. Please use a | as the delimiter. \n",
            " Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.\n",
            "\n",
            "\n",
            "\n",
            "Input: John Smith was 23 years old. He was very tall. He knew Jane Doe. She was 5 years old.\n",
            "Output: first_name|last_name|age\n",
            "John|Smith|23\n",
            "Jane|Doe|5\n",
            "\n",
            "Input: [user input]\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\n",
        "        \"My name is Bob Alice and my phone number is (123)-444-9999. I found my true love one\"\n",
        "        \" on a blue sunday. Her number was (333)1232832. Her name was Moana Sunrise and she was 10 years old.\"\n",
        ")[\"data\"]"
      ],
      "metadata": {
        "id": "l17LdCANwBLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-Uf\""
      ],
      "metadata": {
        "id": "tYmHuVzVAEbc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "I513A3A4AOyd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 长的prompt\n",
        "\n",
        "schema = Object(\n",
        "    id=\"script\",\n",
        "    description=\"Adapted from the novel into script\",\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"role\",\n",
        "            description=\"The character who is speaking or performing an action\",\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"dialogue\",\n",
        "            description=\"The dialogue spoken by the characters in the sentence\",\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"action\",\n",
        "            description='''The actions performed by the characters in the text, A high-level summary of a character's behavior. action equals \"对话\" or \"独白\" if it's dialogue in sentence, equals other action if it's no dialogue''',\n",
        "        )\n",
        "    ],\n",
        "    examples=[\n",
        "        (\n",
        "            '''韦小宝心中闪过一个念头：“我如得了这一千两赏银，我和妈娘儿俩可有得花了，鸡鸭鱼肉，赌钱玩乐，几年也花不光。”见茅十八仍是侧头瞧着自己，脸上神气颇有些古怪，韦小宝怒道：“你心里在想什么你猜我会去通风报信，领这赏银”茅十八道：“是啊，白花花的银子，谁又不爱”''',\n",
        "            [\n",
        "                {\"role\": \"韦小宝\", \"dialogue\": \"我如得了这一千两赏银，我和妈娘儿俩可有得花了，鸡鸭鱼肉，赌钱玩乐，几年也花不光。\",\"action\":\"独白\"},\n",
        "                {\"role\": \"韦小宝\", \"dialogue\": \"你心里在想什么你猜我会去通风报信，领这赏银。\",\"action\":\"对话\"},\n",
        "                {\"role\": \"茅十八\", \"dialogue\": \"是啊，白花花的银子，谁又不爱。\",\"action\":\"对话\"},\n",
        "            ],\n",
        "        ),\n",
        "        (\n",
        "            '''澄观一直站在禅房门口等候。他内力深厚，韦小宝和那女郎的对答，虽微声细语，亦无不入耳，只觉这位师叔“劝说”女施主的言语，委实高深莫测，什么老公、老婆、孙子、爷爷，似乎均与武功无关，正自感佩赞叹，听得他问起解穴之法，忙道：“这位女施主被封的是大包穴，乃属足太阴脾经，师叔替她在腿上箕门、血海两处穴道推血过宫，即可解开。”''',\n",
        "            [\n",
        "                {\"role\": \"澄观\", \"dialogue\": \"\",\"action\":\"站在禅房门口听韦小宝和女郎对话，听到韦小宝询问解穴方法\"},\n",
        "                {\"role\": \"澄观\", \"dialogue\": \"这位女施主被封的是大包穴，乃属足太阴脾经，师叔替她在腿上箕门、血海两处穴道推血过宫，即可解开。\",\"action\":\"对话\"},\n",
        "            ],\n",
        "        )\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DZlfwc5f33v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 短的prompt\n",
        "\n",
        "schema = Object(\n",
        "    id=\"script\",\n",
        "    description=\"Adapted from the novel into script\",\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"role\",\n",
        "            description=\"The character who is speaking or performing an action\",\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"dialogue\",\n",
        "            description=\"The dialogue spoken by the characters in the sentence\",\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"action\",\n",
        "            description='''The actions performed by the characters in the text, A high-level summary of a character's behavior. action equals \"对话\" or \"独白\" if it's dialogue in sentence, equals other action if it's no dialogue''',\n",
        "        )\n",
        "    ],\n",
        "    examples=[\n",
        "        (\n",
        "            '''韦小宝心中闪过一个念头：“我如得了这一千两，我和妈娘儿俩可有得花了” 见茅十八仍是侧头瞧着自己，韦小宝怒道：“你肯定猜我会去通风报信，领这赏银”茅十八道：“是啊，银子谁不爱”''',\n",
        "            [\n",
        "                {\"role\": \"韦小宝\", \"dialogue\": \"我如得了这一千两赏银，我和妈娘儿俩可有得花了\",\"action\":\"独白\"},\n",
        "                {\"role\": \"韦小宝\", \"dialogue\": \"你肯定猜我会去通风报信，领这赏银\",\"action\":\"对话\"},\n",
        "                {\"role\": \"茅十八\", \"dialogue\": \"是啊，银子谁不爱\",\"action\":\"对话\"},\n",
        "            ],\n",
        "        ),\n",
        "        (\n",
        "            '''澄观一直站在禅房门口等候。韦小宝和那女郎的对答，虽微声细语，亦无不入耳，只觉这位师叔“劝说”女施主的言语，委实高深莫测，正自感佩赞叹，听得他问起解穴之法，忙道：“这位女施主被封的是大包穴，师叔替她解开即可”''',\n",
        "            [\n",
        "                {\"role\": \"澄观\", \"dialogue\": \"\",\"action\":\"站在禅房门口听韦小宝和女郎对话，听到韦小宝询问解穴方法\"},\n",
        "                {\"role\": \"澄观\", \"dialogue\": \"这位女施主被封的是大包穴，师叔替她解开即可\",\"action\":\"对话\"},\n",
        "            ],\n",
        "        )\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sLNmvOdpAZix"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(llm, schema)\n",
        "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8KGE31z5oOy",
        "outputId": "2ab78c49-b573-47c1-8efb-ecc593a98830"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your goal is to extract structured information from the user's input that matches the form described below. When extracting information please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
            "\n",
            "```TypeScript\n",
            "\n",
            "script: Array<{ // Adapted from the novel into script\n",
            " role: string // The character who is speaking or performing an action\n",
            " dialogue: string // The dialogue spoken by the characters in the sentence\n",
            " action: string // The actions performed by the characters in the text, A high-level summary of a character's behavior. action equals \"对话\" or \"独白\" if it's dialogue in sentence, equals other action if it's no dialogue\n",
            "}>\n",
            "```\n",
            "\n",
            "\n",
            "Please output the extracted information in CSV format in Excel dialect. Please use a | as the delimiter. \n",
            " Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.\n",
            "\n",
            "\n",
            "\n",
            "Input: 韦小宝心中闪过一个念头：“我如得了这一千两，我和妈娘儿俩可有得花了” 见茅十八仍是侧头瞧着自己，韦小宝怒道：“你肯定猜我会去通风报信，领这赏银”茅十八道：“是啊，银子谁不爱”\n",
            "Output: role|dialogue|action\n",
            "韦小宝|我如得了这一千两赏银，我和妈娘儿俩可有得花了|独白\n",
            "韦小宝|你肯定猜我会去通风报信，领这赏银|对话\n",
            "茅十八|是啊，银子谁不爱|对话\n",
            "\n",
            "Input: 澄观一直站在禅房门口等候。韦小宝和那女郎的对答，虽微声细语，亦无不入耳，只觉这位师叔“劝说”女施主的言语，委实高深莫测，正自感佩赞叹，听得他问起解穴之法，忙道：“这位女施主被封的是大包穴，师叔替她解开即可”\n",
            "Output: role|dialogue|action\n",
            "澄观||站在禅房门口听韦小宝和女郎对话，听到韦小宝询问解穴方法\n",
            "澄观|这位女施主被封的是大包穴，师叔替她解开即可|对话\n",
            "\n",
            "Input: [user input]\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.run(\n",
        "        \"\"\"黄龙使殷锦忙道：“夫人高见。取经之事，想来和福份大小，干系极大。黑龙使也不是不努力。不肯替教主立功，可是始终阻难重重，多半是福气不够，因此宝经难以到手。”洪夫人微笑道：“依你之见，谁的福份够呢”殷锦道：“本教福气最大的，自然是教主他老人家，其次是夫人。不过总不能劳动两位大驾亲自出马。更其次福份最大的，首推白龙使。他识得碣文，又立下大功，印堂隐隐透出红光，福份之大，教主属下无人能出其右。”\n",
        "\n",
        "    教主捻须微笑，道：“但他小小孩童，能担当这件大任么”\n",
        "\n",
        "    白龙使一职，在神龙教虽然甚尊，在韦小宝心里，却半点份量也没有，他既陷身岛上，只好随遇而安，瞧着闭月羞花的洪夫人。自是过瘾之极，但瞧得多了，如给教主发觉自己色迷迷的神色，难免有杀身之祸，还是尽速回北京为妙，听教主这么说，正是脱身的良机，便道：“教主，夫人，承蒙提拔，属下十分感激，我本事是没有的，但托了两位大福气，混进皇宫中去偷这四部宝经，倒也有成功的指望。”洪教主点了点头。洪夫人喜道：“你肯自告奋勇，足见对教主忠心。我知你聪明伶俐，福份又大，恐怕正是上天派来给教主办成这件大事的。”\"\"\"\n",
        ")[\"data\"]"
      ],
      "metadata": {
        "id": "qM6VaR_E_pL1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chat in (response['script']):\n",
        "    print(chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH2uM7E1UvN1",
        "outputId": "91a9d7a7-0db2-4fce-940e-e4fd15ce12ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': '黄龙使殷锦', 'dialogue': '夫人高见。取经之事，想来和福份大小，干系极大。黑龙使也不是不努力。不肯替教主立功，可是始终阻难重重，多半是福气不够，因此宝经难以到手。', 'action': '对话'}\n",
            "{'role': '洪夫人', 'dialogue': '依你之见，谁的福份够呢', 'action': '对话'}\n",
            "{'role': '殷锦', 'dialogue': '本教福气最大的，自然是教主他老人家，其次是夫人。不过总不能劳动两位大驾亲自出马。更其次福份最大的，首推白龙使。他识得碣文，又立下大功，印堂隐隐透出红光，福份之大，教主属下无人能出其右。', 'action': '对话'}\n",
            "{'role': '教主', 'dialogue': '但他小小孩童，能担当这件大任么', 'action': '对话'}\n",
            "{'role': '白龙使', 'dialogue': '教主，夫人，承蒙提拔，属下十分感激，我本事是没有的，但托了两位大福气，混进皇宫中去偷这四部宝经，倒也有成功的指望。', 'action': '对话'}\n",
            "{'role': '洪教主', 'dialogue': '点了点头', 'action': '对话'}\n",
            "{'role': '洪夫人', 'dialogue': '你肯自告奋勇，足见对教主忠心。我知你聪明伶俐，福份又大，恐怕正是上天派来给教主办成这件大事的。', 'action': '对话'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Kpz64jxVNzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们来批量运行下这个代码"
      ],
      "metadata": {
        "id": "bLGyUNRNVOfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAQGV1OBVQVh",
        "outputId": "7540b7bf-37a6-4fc0-c302-0e9b37e6fdd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save_folder = \"/content/drive/MyDrive/GPTData/weixiaobao_extract\"\n",
        "\n",
        "确认这个folder的存在性，如果不存在这个folder，则新建这个folder"
      ],
      "metadata": {
        "id": "V0iqMBwlVy_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_folder = \"/content/drive/MyDrive/GPTData/weixiaobao_extract\"\n",
        "\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)"
      ],
      "metadata": {
        "id": "CnnOHg9tVZRV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/Prophet-Andrew-Ng/main/langchain/%E9%B9%BF%E9%BC%8E%E8%AE%B0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nm1Uuk3V9zd",
        "outputId": "c062bf04-a81f-44d9-dd2d-83847f962ed6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-28 07:13:39--  https://raw.githubusercontent.com/LC1332/Prophet-Andrew-Ng/main/langchain/%E9%B9%BF%E9%BC%8E%E8%AE%B0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3701970 (3.5M) [text/plain]\n",
            "Saving to: ‘鹿鼎记.txt’\n",
            "\n",
            "鹿鼎记.txt          100%[===================>]   3.53M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-07-28 07:13:40 (40.3 MB/s) - ‘鹿鼎记.txt’ saved [3701970/3701970]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read /content/鹿鼎记.txt into python string raw_text\n",
        "with open('/content/鹿鼎记.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "hWfr_Y8sWLNJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据\\n将 raw_text的文本切开，存储到split_text中，用python实现"
      ],
      "metadata": {
        "id": "xTwe_-i4WeyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_text = raw_text.split('\\n')"
      ],
      "metadata": {
        "id": "DQWdB0gsWaUQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(split_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY6uOb01WstC",
        "outputId": "7d062b9e-4053-4d28-c945-05f7321be69a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tiktoken"
      ],
      "metadata": {
        "id": "8IyZrBpgW0Jn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "h-C1RNpKW7oe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split_text中存储着一个python的list of string\n",
        "\n",
        "已知 len(enc.encode( my_str )) 会计算my_str的token长度\n",
        "\n",
        "定义max_token_len = 1000\n",
        "\n",
        "生成一个新的数组chunk_text\n",
        "\n",
        "里面的每个元素分别是split_text的顺序拼接\n",
        "\n",
        "保证每个chunk_text的长度不要超过max_token_len"
      ],
      "metadata": {
        "id": "UNq4LCUcXAIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_token_len = 1000\n",
        "chunk_text = []\n",
        "\n",
        "curr_len = 0\n",
        "curr_chunk = ''\n",
        "\n",
        "for line in split_text:\n",
        "  line_len = len(enc.encode( line ))\n",
        "\n",
        "  if line_len > max_token_len:\n",
        "    print('warning line_len = ', line_len)\n",
        "\n",
        "  if curr_len + line_len <= max_token_len:\n",
        "    curr_chunk += line\n",
        "    curr_chunk += '\\n'\n",
        "    curr_len += line_len\n",
        "    curr_len += 1\n",
        "  else:\n",
        "    chunk_text.append(curr_chunk)\n",
        "    curr_chunk = line\n",
        "    curr_len = line_len\n",
        "\n",
        "if curr_chunk:\n",
        "  chunk_text.append(curr_chunk)"
      ],
      "metadata": {
        "id": "XLbrbDVdXl97"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunk_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-EMviXjXzXr",
        "outputId": "ab37aa11-775a-4b35-d860-28b40595978e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunk_text[123])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACsB7DthYD-o",
        "outputId": "db5d7e57-3482-4485-f58f-0de7c446466c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    韦小宝回到屋中，向海老公道：“公公，你的法子不管用，太也稀松平常。”海老公哼了一声，说道：“没出息，又打输了。”韦小宝道：“如果用我自己的法子，虽然不一定准赢，也不见得准输。可是你的法子太也脓包，人家也都会的，有什么希奇”海老公奇道：“他也知道这法子你试给我瞧瞧。”韦小宝心想：“你眼睛瞎了，试给你看看，难道你看得见么”突然心念一动：“不知他是真瞎还是假瞎，可得试他一试。”当即双肘向后一撞，道：“他这么一撞，只撞得我全身三千根骨头，根根都痛。”海老公叹了口气，道：“你说这么一撞，我又怎瞧得见”颤巍巍的站起身来，道：“你试着学他的样。”韦小宝心下暗喜：“老乌龟是真的瞎了。”背心向着他，挺肘缓缓向后撞去，道：“他用手肘这样撞我。”待得手肘碰到了海老公胸口，便不再使力。海老公嗯了一声，说道：“这是腋底锤，那也算不了什么。”韦小宝道：“还有这样。”拉住了海老公左手，放在自己右肩，说道：“他用力一甩，我身子便从他头顶飞了过去。”这一招其实是他甩倒小玄子的得意之作，故意倒转来说，要考一考海老公。海老公道：“这是羚羊挂角。”韦小宝道：“原来你早知道了。”跟着拉住他手臂，慢慢而后拗转。海老公道：“嗯，这是倒折梅中的第三手。还有什么”\n",
            "    韦小宝道：“原来小玄子这些手法都有名堂，我跟他乱打乱扭，那些手段可也得有几个好听的名堂才成啊。我向他扑过去，这小子向旁闪开，却在我背上顺势一推，我就”海老公不等他说完，便问：“他推在你哪里”韦小宝道：“他一推我便摔得七荤八素，怎还记得推在哪里。”海老公道：“你记记看。是推在这里么”说着伸手按在他左肩背后。韦小宝道：“不是。”海老公道：“是这里么”按在他右肩背后。韦小宝仍道：“不是。”海老公连按了六七个部位，韦小宝都说不是。海老公伸掌按在他右腰肋骨之下，问道：“是这里么”说着轻轻一推。韦小宝一个踉跄，跌出几步，立时记起小玄子推他的正是这个所在，大声道：“是了，一点不错，正是这里。公公，你怎么知道”\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.run( chunk_text[123] )[\"data\"]"
      ],
      "metadata": {
        "id": "9QCKCgN-YNQR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chat in (response['script']):\n",
        "    print(chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZjZXICyYVIi",
        "outputId": "003ba896-877f-4176-a775-6f9aafbb473d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': '韦小宝', 'dialogue': '公公，你的法子不管用，太也稀松平常', 'action': '对话'}\n",
            "{'role': '海老公', 'dialogue': '没出息，又打输了', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '如果用我自己的法子，虽然不一定准赢，也不见得准输。可是你的法子太也脓包，人家也都会的，有什么希奇', 'action': '对话'}\n",
            "{'role': '海老公', 'dialogue': '他也知道这法子你试给我瞧瞧', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '他这么一撞，只撞得我全身三千根骨头，根根都痛', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '你说这么一撞，我又怎瞧得见', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '你试着学他的样', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '他用手肘这样撞我', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '这是腋底锤，那也算不了什么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '还有这样', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '他用力一甩，我身子便从他头顶飞了过去', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '这是羚羊挂角', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '原来你早知道了', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '他一推我便摔得七荤八素，怎还记得推在哪里', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '你记记看。是推在这里么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '不是', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '是这里么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '不是', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '是这里么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '不是', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '是这里么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '不是', 'action': '独白'}\n",
            "{'role': '海老公', 'dialogue': '是这里么', 'action': '对话'}\n",
            "{'role': '韦小宝', 'dialogue': '是了，一点不错，正是这里。公公，你怎么知道', 'action': '对话'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4LRMzfCvZnT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(chunk_text)):\n",
        ""
      ],
      "metadata": {
        "id": "Xhtyi5HYZZG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "已知\n",
        "for chat in (response['script']):\n",
        "    print(chat)\n",
        "\n",
        "的输出结果如下\n",
        "\n",
        "{'role': '韦小宝', 'dialogue': '公公，你的法子不管用，太也稀松平常', 'action': '对话'}\n",
        "{'role': '海老公', 'dialogue': '没出息，又打输了', 'action': '对话'}\n",
        "{'role': '韦小宝', 'dialogue': '如果用我自己的法子，虽然不一定准赢，也不见得准输。可是你的法子太也脓包，人家也都会的，有什么希奇', 'action': '对话'}\n",
        "{'role': '海老公', 'dialogue': '他也知道这法子你试给我瞧瞧', 'action': '对话'}\n",
        "{'role': '韦小宝', 'dialogue': '他这么一撞，只撞得我全身三千根骨头，根根都痛', 'action': '独白'}\n",
        "\n",
        "定义save_name = 'output.jsonl'\n",
        "\n",
        "将程序改写为把response['script']中的内容保存到save_name"
      ],
      "metadata": {
        "id": "59U2lbEBYiY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# save_name = '/content/test_output.jsonl'\n",
        "\n",
        "# with open(save_name, 'w', encoding='utf-8') as f:\n",
        "\n",
        "#   for chat in response['script']:\n",
        "\n",
        "#     json_str = json.dumps(chat, ensure_ascii=False)\n",
        "#     f.write(json_str+\"\\n\")"
      ],
      "metadata": {
        "id": "-yeeomNqY3zV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "save_folder = \"/content/drive/MyDrive/GPTData/weixiaobao_extract\"\n",
        "\n",
        "i从0到100循环\n",
        "\n",
        "根据数字i 组织一个jsonl的保存文件名\n",
        "\n",
        "如果文件名对应的文件已经存在，则跳过当前步骤\n",
        "\n",
        "不然则运行\n",
        "\n",
        "```python\n",
        "with open(save_name, 'w', encoding='utf-8') as f:\n",
        "\n",
        "  for chat in response['script']:\n",
        "    \n",
        "    json_str = json.dumps(chat, ensure_ascii=False)\n",
        "    f.write(json_str+\"\\n\")\n",
        "```\n",
        "\n",
        "进行写文件\n",
        "\n",
        "整体的循环再增加一个进度条提示\n"
      ],
      "metadata": {
        "id": "MPrbgx8HZ5By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "save_folder = \"/content/drive/MyDrive/GPTData/weixiaobao_extract\"\n",
        "\n",
        "for i in tqdm(range(29,len(chunk_text))):\n",
        "\n",
        "  save_name = os.path.join(save_folder, f\"{i}.txt\")\n",
        "\n",
        "  if os.path.exists(save_name):\n",
        "    continue\n",
        "\n",
        "  response = chain.run( chunk_text[i] )[\"data\"]\n",
        "\n",
        "  with open(save_name, 'w', encoding='utf-8') as f:\n",
        "\n",
        "    if 'script' not in response:\n",
        "        print('Error: response does not contain key \"script\"')\n",
        "    else:\n",
        "        for chat in response['script']:\n",
        "            json_str = json.dumps(chat, ensure_ascii=False)\n",
        "            f.write(json_str+\"\\n\")\n",
        "\n",
        "\n",
        "#   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJgsmT_4aHOi",
        "outputId": "3c0b1c8c-7da6-41f1-b81f-042ebca9ef44"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 33/1847 [07:26<9:27:51, 18.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 44/1847 [10:33<9:33:28, 19.08s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 07:59:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7edb8cfdc8b149fa-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            "  6%|▌         | 105/1847 [34:02<10:06:24, 20.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 131/1847 [42:18<5:48:16, 12.18s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            "  8%|▊         | 148/1847 [46:50<7:23:31, 15.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 168/1847 [52:23<8:26:44, 18.11s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 10%|█         | 186/1847 [58:02<8:54:26, 19.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 205/1847 [1:03:27<9:04:34, 19.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 226/1847 [1:09:02<9:04:50, 20.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 245/1847 [1:14:29<8:26:02, 18.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 246/1847 [1:14:52<9:03:16, 20.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▎        | 251/1847 [1:16:54<10:11:00, 22.97s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 14%|█▎        | 252/1847 [1:17:34<12:33:09, 28.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 258/1847 [1:19:17<7:40:58, 17.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 270/1847 [1:22:50<8:54:55, 20.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▋        | 301/1847 [1:32:09<6:06:39, 14.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 302/1847 [1:32:40<8:15:13, 19.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 315/1847 [1:36:02<6:52:58, 16.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 328/1847 [1:39:31<8:49:57, 20.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 342/1847 [1:42:29<5:30:05, 13.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 345/1847 [1:43:25<7:21:56, 17.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 349/1847 [1:44:03<4:53:10, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 365/1847 [1:48:30<6:56:48, 16.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 385/1847 [1:53:56<5:40:36, 13.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 391/1847 [1:55:58<8:32:10, 21.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 401/1847 [1:59:32<9:14:26, 23.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 411/1847 [2:02:39<7:58:46, 20.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 417/1847 [2:04:29<7:11:18, 18.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 420/1847 [2:05:44<9:16:08, 23.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 427/1847 [2:08:18<8:35:09, 21.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 429/1847 [2:09:03<8:38:33, 21.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 432/1847 [2:09:58<7:19:34, 18.64s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 09:59:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7edc3cae8bc64a52-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 24%|██▍       | 446/1847 [2:20:10<6:48:39, 17.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 449/1847 [2:21:11<7:40:03, 19.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 480/1847 [2:29:24<5:30:44, 14.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 482/1847 [2:29:59<6:26:57, 17.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 483/1847 [2:30:16<6:23:33, 16.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 498/1847 [2:34:07<6:05:01, 16.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 500/1847 [2:34:40<6:14:01, 16.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 533/1847 [2:44:16<6:22:13, 17.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 548/1847 [2:47:53<5:36:22, 15.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 550/1847 [2:48:29<6:11:14, 17.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 560/1847 [2:50:50<5:10:46, 14.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 574/1847 [2:55:03<11:08:56, 31.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 640/1847 [3:10:36<3:49:15, 11.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 641/1847 [3:11:10<6:05:01, 18.16s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 35%|███▌      | 647/1847 [3:13:17<5:33:38, 16.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 658/1847 [3:15:33<4:01:41, 12.20s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 36%|███▌      | 664/1847 [3:17:37<4:48:46, 14.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 669/1847 [3:18:48<4:53:22, 14.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 684/1847 [3:22:21<5:04:21, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 689/1847 [3:23:34<4:47:04, 14.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 711/1847 [3:28:55<5:17:38, 16.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 716/1847 [3:30:02<4:54:56, 15.65s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 39%|███▉      | 717/1847 [3:31:01<8:56:59, 28.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 733/1847 [3:34:37<4:12:50, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 745/1847 [3:37:43<6:07:57, 20.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 751/1847 [3:38:54<4:19:34, 14.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 756/1847 [3:40:21<5:44:56, 18.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 758/1847 [3:40:58<5:33:25, 18.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 773/1847 [3:44:33<3:36:41, 12.11s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 42%|████▏     | 774/1847 [3:45:32<7:45:10, 26.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 775/1847 [3:45:57<7:42:11, 25.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 794/1847 [3:51:18<4:44:35, 16.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 799/1847 [3:52:43<4:52:20, 16.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 802/1847 [3:53:49<6:04:43, 20.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 812/1847 [3:56:44<4:37:23, 16.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 823/1847 [3:59:09<3:57:37, 13.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 830/1847 [4:01:24<5:57:20, 21.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 835/1847 [4:02:35<4:38:56, 16.54s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 45%|████▌     | 836/1847 [4:03:28<7:46:09, 27.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 838/1847 [4:04:22<7:39:15, 27.31s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 46%|████▌     | 842/1847 [4:05:56<5:43:05, 20.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 843/1847 [4:06:16<5:41:46, 20.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 872/1847 [4:15:24<4:06:46, 15.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 888/1847 [4:20:21<4:53:42, 18.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▊     | 896/1847 [4:22:43<4:23:04, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 924/1847 [4:30:13<4:26:04, 17.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 934/1847 [4:32:41<3:28:47, 13.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 942/1847 [4:35:29<5:17:52, 21.07s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 12:24:22 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7edd11132eee4a01-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 52%|█████▏    | 958/1847 [4:45:59<4:37:49, 18.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 962/1847 [4:47:31<5:25:25, 22.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 970/1847 [4:50:08<5:01:01, 20.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 972/1847 [4:50:32<3:56:49, 16.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▎    | 989/1847 [4:56:40<6:14:23, 26.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▎    | 992/1847 [4:57:36<4:47:48, 20.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 54%|█████▍    | 1000/1847 [5:01:04<5:59:40, 25.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 1014/1847 [5:05:56<5:01:40, 21.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▋    | 1041/1847 [5:15:40<4:26:58, 19.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 1052/1847 [5:18:33<3:51:48, 17.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 1065/1847 [5:22:59<4:37:15, 21.27s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 58%|█████▊    | 1080/1847 [5:28:22<4:25:35, 20.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▊    | 1085/1847 [5:31:12<6:00:06, 28.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 1086/1847 [5:31:43<6:08:40, 29.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 1091/1847 [5:33:09<4:27:29, 21.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 1098/1847 [5:35:24<4:11:37, 20.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1110/1847 [5:39:59<4:40:20, 22.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 1120/1847 [5:43:21<4:02:15, 19.99s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 61%|██████    | 1129/1847 [5:47:06<3:41:35, 18.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 1132/1847 [5:47:59<3:32:26, 17.83s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 61%|██████▏   | 1134/1847 [5:48:59<4:18:55, 21.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1138/1847 [5:50:16<4:23:27, 22.30s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 62%|██████▏   | 1142/1847 [5:51:52<4:21:44, 22.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 1159/1847 [5:57:47<3:46:21, 19.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 1164/1847 [5:59:06<3:03:59, 16.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 1168/1847 [6:00:40<4:09:16, 22.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 1179/1847 [6:03:31<3:10:40, 17.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 1185/1847 [6:05:35<4:10:05, 22.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 1196/1847 [6:09:07<3:54:34, 21.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 1197/1847 [6:09:38<4:26:16, 24.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 1198/1847 [6:09:58<4:10:19, 23.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1201/1847 [6:10:47<3:20:48, 18.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1206/1847 [6:12:22<3:20:45, 18.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1209/1847 [6:13:46<4:20:09, 24.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 1210/1847 [6:14:11<4:20:00, 24.49s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 66%|██████▌   | 1212/1847 [6:15:44<6:04:10, 34.41s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 66%|██████▌   | 1221/1847 [6:19:39<4:30:32, 25.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1239/1847 [6:25:54<3:07:12, 18.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1241/1847 [6:26:33<3:11:18, 18.94s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 67%|██████▋   | 1242/1847 [6:27:28<4:58:58, 29.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1246/1847 [6:28:28<3:22:47, 20.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 1249/1847 [6:29:10<2:43:01, 16.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 1256/1847 [6:32:18<4:14:33, 25.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 1273/1847 [6:37:35<2:05:46, 13.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 1282/1847 [6:40:20<3:06:29, 19.80s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 14:28:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7eddc7f32f284a6d-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 70%|███████   | 1299/1847 [6:50:47<2:41:58, 17.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1302/1847 [6:51:26<2:15:28, 14.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 1304/1847 [6:51:53<2:09:30, 14.31s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 71%|███████   | 1308/1847 [6:53:49<3:29:22, 23.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████▏  | 1317/1847 [6:56:43<3:24:16, 23.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 1321/1847 [6:58:24<3:28:51, 23.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 1325/1847 [7:00:08<3:52:19, 26.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 1333/1847 [7:03:25<3:18:37, 23.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 1344/1847 [7:06:54<2:53:39, 20.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 1348/1847 [7:07:57<2:20:36, 16.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 1358/1847 [7:11:44<2:53:54, 21.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 1360/1847 [7:12:39<3:21:24, 24.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 1367/1847 [7:14:36<2:05:37, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1388/1847 [7:21:13<2:51:18, 22.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▋  | 1410/1847 [7:28:26<2:56:03, 24.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 15:17:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ede0e684cf54a6d-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 76%|███████▋  | 1411/1847 [7:34:27<15:11:57, 125.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 1413/1847 [7:35:23<9:08:06, 75.78s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 1419/1847 [7:37:02<2:46:10, 23.30s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 78%|███████▊  | 1432/1847 [7:40:54<1:43:41, 14.99s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 15:29:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ede20ae39f14a85-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 78%|███████▊  | 1433/1847 [7:46:35<12:57:50, 112.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 1437/1847 [7:48:16<5:24:19, 47.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 1438/1847 [7:48:23<4:01:58, 35.50s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 78%|███████▊  | 1440/1847 [7:49:45<4:12:31, 37.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 1444/1847 [7:50:58<2:33:45, 22.89s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: HTTP code 502 from API (<html>\n",
            "<head><title>502 Bad Gateway</title></head>\n",
            "<body>\n",
            "<center><h1>502 Bad Gateway</h1></center>\n",
            "<hr><center>cloudflare</center>\n",
            "</body>\n",
            "</html>\n",
            ").\n",
            " 78%|███████▊  | 1448/1847 [7:52:36<2:25:55, 21.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▊  | 1454/1847 [7:55:00<2:15:59, 20.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 1474/1847 [8:00:02<1:54:40, 18.45s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 80%|███████▉  | 1476/1847 [8:01:53<3:29:43, 33.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1492/1847 [8:06:18<1:29:22, 15.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 1496/1847 [8:07:24<1:32:40, 15.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████▏ | 1503/1847 [8:09:15<1:38:59, 17.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1511/1847 [8:11:57<2:03:17, 22.02s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 16:00:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ede4e290cba4a7e-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 82%|████████▏ | 1520/1847 [8:19:58<2:18:48, 25.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 1528/1847 [8:22:38<1:41:46, 19.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 1541/1847 [8:27:20<2:01:17, 23.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1544/1847 [8:28:05<1:29:44, 17.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 1550/1847 [8:30:05<1:40:55, 20.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 1551/1847 [8:30:35<1:54:59, 23.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 1553/1847 [8:31:07<1:39:17, 20.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 1569/1847 [8:35:17<1:18:59, 17.05s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 28 Jul 2023 16:23:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ede7055ca446a8d-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            " 85%|████████▌ | 1575/1847 [8:41:57<2:23:11, 31.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 1618/1847 [8:53:26<1:01:22, 16.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 1627/1847 [8:57:06<1:16:23, 20.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 1644/1847 [9:01:00<47:26, 14.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 1648/1847 [9:02:32<1:05:24, 19.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 1657/1847 [9:05:03<1:02:24, 19.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 1661/1847 [9:06:10<59:02, 19.04s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 90%|████████▉ | 1662/1847 [9:06:57<1:25:01, 27.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1664/1847 [9:07:57<1:27:49, 28.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1665/1847 [9:08:21<1:22:31, 27.21s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 91%|█████████ | 1676/1847 [9:12:46<1:05:06, 22.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 1683/1847 [9:15:06<1:03:22, 23.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████▏| 1688/1847 [9:17:03<1:07:20, 25.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1702/1847 [9:21:54<42:01, 17.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 1717/1847 [9:25:14<42:25, 19.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▎| 1727/1847 [9:29:02<54:25, 27.21s/it]WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            " 94%|█████████▎| 1730/1847 [9:29:54<37:18, 19.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 1771/1847 [9:41:15<19:56, 15.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 1776/1847 [9:43:02<23:01, 19.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 1778/1847 [9:43:35<20:57, 18.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 1780/1847 [9:44:05<19:28, 17.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 1797/1847 [9:48:35<15:25, 18.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 1803/1847 [9:50:10<12:33, 17.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 1818/1847 [9:55:15<08:49, 18.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 1827/1847 [9:58:25<06:55, 20.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1839/1847 [10:01:31<02:15, 16.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|█████████▉| 1840/1847 [10:01:46<01:55, 16.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: response does not contain key \"script\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1847/1847 [10:03:08<00:00, 19.59s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2DF38xvc4lV",
        "outputId": "0a65c455-399b-4891-a1cd-9a6a3b6573d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 抽取哈利波特"
      ],
      "metadata": {
        "id": "0NDljCJA_nEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai google-search-results tiktoken\n",
        "!pip -q install kor markdownify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7rAC_HU_pBh",
        "outputId": "70351729-a223-4068-8d2c-b72baf3f3f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kor Basics\n",
        "\n",
        "The basic workflow is the following:\n",
        "\n",
        "1. Load the document\n",
        "2. Clean up the document (optional)\n",
        "3. Split the document into chunks\n",
        "4. Define a schema for extraction\n",
        "5. Extract from every chunk of text"
      ],
      "metadata": {
        "id": "9M-CROtIBixD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from kor.extraction import create_extraction_chain\n",
        "from kor.nodes import Object, Text, Number\n",
        "\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from kor import extract_from_documents, from_pydantic, create_extraction_chain\n",
        "\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "Kk3c8z0PBgeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple examples"
      ],
      "metadata": {
        "id": "29nRABvafCEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        ")"
      ],
      "metadata": {
        "id": "sJiJwJ5UfER6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = Object(\n",
        "    id=\"personal_info\",\n",
        "    description=\"Personal information about a given person.\",\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"first_name\",\n",
        "            description=\"The first name of the person\",\n",
        "            examples=[(\"John Smith went to the store\", \"John\")],\n",
        "        ),\n",
        "        Text(\n",
        "            id=\"last_name\",\n",
        "            description=\"The last name of the person\",\n",
        "            examples=[(\"John Smith went to the store\", \"Smith\")],\n",
        "        ),\n",
        "        Number(\n",
        "            id=\"age\",\n",
        "            description=\"The age of the person in years.\",\n",
        "            examples=[(\"23 years old\", \"23\"), (\"I turned three on sunday\", \"3\")],\n",
        "        ),\n",
        "    ],\n",
        "    examples=[\n",
        "        (\n",
        "            \"John Smith was 23 years old. He was very tall. He knew Jane Doe. She was 5 years old.\",\n",
        "            [\n",
        "                {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 23},\n",
        "                {\"first_name\": \"Jane\", \"last_name\": \"Doe\", \"age\": 5},\n",
        "            ],\n",
        "        )\n",
        "    ],\n",
        "    many=True,\n",
        ")\n",
        "\n",
        "\n",
        "chain = create_extraction_chain(llm, schema)"
      ],
      "metadata": {
        "id": "LDXQgLLVfd73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWTguilQfkTM",
        "outputId": "d1d35d88-4086-4f73-9592-e32984b5ece1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your goal is to extract structured information from the user's input that matches the form described below. When extracting information please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
            "\n",
            "```TypeScript\n",
            "\n",
            "personal_info: Array<{ // Personal information about a given person.\n",
            " first_name: string // The first name of the person\n",
            " last_name: string // The last name of the person\n",
            " age: number // The age of the person in years.\n",
            "}>\n",
            "```\n",
            "\n",
            "\n",
            "Please output the extracted information in CSV format in Excel dialect. Please use a | as the delimiter. \n",
            " Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.\n",
            "\n",
            "\n",
            "\n",
            "Input: John Smith was 23 years old. He was very tall. He knew Jane Doe. She was 5 years old.\n",
            "Output: first_name|last_name|age\n",
            "John|Smith|23\n",
            "Jane|Doe|5\n",
            "\n",
            "Input: John Smith went to the store\n",
            "Output: first_name|last_name|age\n",
            "John||\n",
            "\n",
            "Input: John Smith went to the store\n",
            "Output: first_name|last_name|age\n",
            "|Smith|\n",
            "\n",
            "Input: 23 years old\n",
            "Output: first_name|last_name|age\n",
            "||23\n",
            "\n",
            "Input: I turned three on sunday\n",
            "Output: first_name|last_name|age\n",
            "||3\n",
            "\n",
            "Input: [user input]\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.predict_and_parse(text=\"David Jones was 34 years old a long time ago.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "4PjSryvAfwXE",
        "outputId": "37533634-bbcf-436f-c9c0-84499314c026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f5adf3ed0ef7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_and_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"David Jones was 34 years old a long time ago.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kor/extraction/parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\"Parse the text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"errors\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validated_data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kor/encoders/csv_data.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtable_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     df = pd.read_csv(\n",
            "\u001b[0;31mTypeError\u001b[0m: initial_value must be str or None, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested Objects and JSON"
      ],
      "metadata": {
        "id": "dz9gWTk4TGd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from_address = Object(\n",
        "    id=\"from_address\",\n",
        "    description=\"Person moved away from this address\",\n",
        "    attributes=[\n",
        "        Text(id=\"street\"),\n",
        "        Text(id=\"city\"),\n",
        "        Text(id=\"state\"),\n",
        "        Text(id=\"zipcode\"),\n",
        "        Text(id=\"country\", description=\"A country in the world; e.g., France.\"),\n",
        "    ],\n",
        "    examples=[\n",
        "        (\n",
        "            \"100 Main St, Boston, MA, 23232, USA\",\n",
        "            {\n",
        "                \"street\": \"100 Marlo St\",\n",
        "                \"city\": \"Boston\",\n",
        "                \"state\": \"MA\",\n",
        "                \"zipcode\": \"23232\",\n",
        "                \"country\": \"USA\",\n",
        "            },\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "to_address = from_address.replace(\n",
        "    id=\"to_address\", description=\"Address to which the person is moving\"\n",
        ")\n",
        "\n",
        "schema = Object(\n",
        "    id=\"information\",\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"person_name\",\n",
        "            description=\"The full name of the person or partial name\",\n",
        "            examples=[(\"John Smith was here\", \"John Smith\")],\n",
        "        ),\n",
        "        from_address,\n",
        "        to_address,\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "LOOSR4DyYX-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON encoding\n",
        "To use nested objects, at least for now we have to swap to the JSON encoder."
      ],
      "metadata": {
        "id": "f37I2uk5ezfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(\n",
        "    llm, schema, encoder_or_encoder_class=\"json\", input_formatter=None\n",
        ")"
      ],
      "metadata": {
        "id": "UpcrZoeje1Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-7Pox8hulAX",
        "outputId": "7520ccf5-d6f7-43f8-f7fc-832d1fb4a024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your goal is to extract structured information from the user's input that matches the form described below. When extracting information please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
            "\n",
            "```TypeScript\n",
            "\n",
            "information: Array<{ // \n",
            " person_name: string // The full name of the person or partial name\n",
            " to_address: { // Address to which the person is moving\n",
            "  street: string // \n",
            "  city: string // \n",
            "  state: string // \n",
            "  zipcode: string // \n",
            "  country: string // A country in the world; e.g., France.\n",
            " }\n",
            " to_address: { // Address to which the person is moving\n",
            "  street: string // \n",
            "  city: string // \n",
            "  state: string // \n",
            "  zipcode: string // \n",
            "  country: string // A country in the world; e.g., France.\n",
            " }\n",
            "}>\n",
            "```\n",
            "\n",
            "\n",
            "Please output the extracted information in JSON format. Do not output anything except for the extracted information. Do not add any clarifying information. Do not add any fields that are not in the schema. If the text contains attributes that do not appear in the schema, please ignore them. All output must be in JSON format and follow the schema specified above. Wrap the JSON in <json> tags.\n",
            "\n",
            "\n",
            "\n",
            "Input: John Smith was here\n",
            "Output: <json>{\"information\": [{\"person_name\": \"John Smith\"}]}</json>\n",
            "Input: 100 Main St, Boston, MA, 23232, USA\n",
            "Output: <json>{\"information\": [{\"to_address\": {\"street\": \"100 Marlo St\", \"city\": \"Boston\", \"state\": \"MA\", \"zipcode\": \"23232\", \"country\": \"USA\"}}]}</json>\n",
            "Input: 100 Main St, Boston, MA, 23232, USA\n",
            "Output: <json>{\"information\": [{\"to_address\": {\"street\": \"100 Marlo St\", \"city\": \"Boston\", \"state\": \"MA\", \"zipcode\": \"23232\", \"country\": \"USA\"}}]}</json>\n",
            "Input: [user input]\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.predict_and_parse(\n",
        "    text=\"Alice Doe moved from New York to Boston, MA while Bob Smith did the opposite.\"\n",
        ")[\"data\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "Pa4Jdj89khtD",
        "outputId": "a9f2acb0-1252-469a-c830-f53bc09ee5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-295d54d87229>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m chain.predict_and_parse(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Alice Doe moved from New York to Boston, MA while Bob Smith did the opposite.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )[\"data\"]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kor/extraction/parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\"Parse the text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"errors\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validated_data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kor/encoders/json_data.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \"\"\"\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kor/encoders/utils.py\u001b[0m in \u001b[0;36munwrap_tag\u001b[0;34m(tag_name, text)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"<{tag_name}>(.*?)</{tag_name}>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOTALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Pydantic and validation"
      ],
      "metadata": {
        "id": "jZRRn5mxQPDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.dropbox.com/s/gekyuep86zibhl1/conversation-025722052023.txt"
      ],
      "metadata": {
        "id": "6s4_vP1DfSd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the document"
      ],
      "metadata": {
        "id": "9PX7S0ykCJ9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conversation(filename):\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        conversation = f.read()\n",
        "\n",
        "    return conversation\n"
      ],
      "metadata": {
        "id": "2EpriZKOhbHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = load_conversation('/content/conversation-025722052023.txt')\n",
        "\n",
        "len(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yJMEUnQt9id",
        "outputId": "d21d42d4-fdc3-47aa-aa16-730c763cf42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9456"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "B7TLMT90rMdo",
        "outputId": "4b4d81f3-de36-4ffa-d726-d029a28aa52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Food lover 2: Instruction: Please describe your first most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My first most unforgettable meal was at a restaurant called El Celler de Can Roca in Girona, Spain. The ambiance was elegant and modern, and the food was a creative and delicious 18-course tasting menu. One unique experience was when they brought out a dish that was inspired by the smells of the forest.\\n\\nFood lover 1: My response: That sounds amazing! The forest-inspired dish must have been a unique experience. My first most unforgettable meal was at a restaurant called Noma in Copenhagen, Denmark. The location was in an old warehouse by the waterfront, and the ambiance was rustic and cozy. The food was presented in a simple and natural way, with many of the ingredients sourced from the surrounding Nordic region. One of the most memorable dishes was a dessert made with fermented berries and ants, which added a surprising and delicious crunch. Overall, it was a truly unforgettable dining experience that showcased the beauty and flavors of Nordic cuisine. Next question.\\n\\nFood lover 2: Instruction: Please describe your second most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My second most unforgettable meal was at a small family-owned restaurant called La Cava del Tequila in Mexico City. The ambiance was cozy and intimate, with dim lighting and traditional Mexican decor. The food was authentic and flavorful, with a focus on regional specialties. One unique experience was when the owner came to our table and gave us a tequila tasting, explaining the different types and flavors. The standout dish for me was the mole, which had a complex and rich flavor that I had never tasted before.\\n\\nFood lover 1: My response: That sounds like a wonderful experience! My second most unforgettable meal was at a restaurant called Gaggan in Bangkok, Thailand. The location was in a beautiful colonial-style house, and the ambiance was modern and chic. The food was a creative and playful take on Indian cuisine, with each dish presented in a unique and artistic way. One unique experience was when they brought out a dish that was meant to be eaten with your hands, and it was served on a miniature replica of a tree. The standout dish for me was the \"Lick It Up\" course, which was a deconstructed version of a traditional Indian street food dish. Overall, it was an unforgettable dining experience that showcased the creativity and innovation of modern Indian cuisine. Next question.\\n\\nFood lover 2: Instruction: Please describe your third most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My third most unforgettable meal was at a restaurant called Osteria Francescana in Modena, Italy. The location was in a historic building in the heart of the city, and the ambiance was elegant and refined. The food was a modern take on traditional Italian cuisine, with a focus on local and seasonal ingredients. One unique experience was when they brought out a dish that was inspired by the childhood memories of the chef, Massimo Bottura. The standout dish for me was the \"Oops! I Dropped the Lemon Tart,\" which was a deconstructed version of a lemon tart that was presented in a playful and unexpected way. Overall, it was an unforgettable dining experience that showcased the creativity and passion of Italian cuisine.\\n\\nFood lover 1: My response: That\\'s a great choice! My third most unforgettable meal was at a restaurant called Attica in Melbourne, Australia. The location was in a converted warehouse, and the ambiance was modern and minimalist. The food was a creative and innovative take on Australian cuisine, with a focus on native ingredients and flavors. One unique experience was when they brought out a dish that was meant to be eaten with your hands, and it was served on a bed of eucalyptus leaves. The standout dish for me was the \"Potato cooked in the earth it was grown,\" which was a simple yet delicious dish that showcased the natural flavors of the potato. Overall, it was an unforgettable dining experience that showcased the beauty and diversity of Australian cuisine. Next question.\\n\\nFood lover 2: Instruction: Please rate my first meal based on originality, presentation, and overall satisfaction.\\nInput: I would rate your first meal very highly based on originality, presentation, and overall satisfaction. The forest-inspired dish sounds like a unique and creative concept, and the elegant and modern ambiance of the restaurant must have added to the overall experience. The 18-course tasting menu also shows a high level of creativity and attention to detail. Overall, it sounds like a truly unforgettable dining experience.\\n\\nFood lover 1: My response: I completely agree with your assessment. Your first meal sounds like it was a truly unforgettable dining experience, with a high level of originality, presentation, and overall satisfaction. The attention to detail and creativity in the dishes, as well as the elegant and modern ambiance of the restaurant, must have made for a truly memorable experience. I would rate your first meal very highly based on these factors. Next question.\\n\\nFood lover 2: Instruction: Please rate my second meal based on originality, presentation, and overall satisfaction.\\nInput: I would rate your second meal very highly based on originality, presentation, and overall satisfaction. The cozy and intimate ambiance of the restaurant, along with the authentic and flavorful Mexican cuisine, must have made for a truly memorable experience. The tequila tasting and focus on regional specialties also show a high level of attention to detail and passion for the cuisine. Overall, it sounds like a wonderful dining experience.\\n\\nFood lover 1: My response: I completely agree with your assessment. Your second meal sounds like it was a wonderful dining experience, with a high level of originality, presentation, and overall satisfaction. The cozy and intimate ambiance of the restaurant, along with the authentic and flavorful Mexican cuisine, must have made for a truly memorable experience. The tequila tasting and focus on regional specialties also show a high level of attention to detail and passion for the cuisine. I would rate your second meal very highly based on these factors. Next question.\\n\\nFood lover 2: Instruction: Please rate my third meal based on originality, presentation, and overall satisfaction.\\nInput: I would rate your third meal very highly based on originality, presentation, and overall satisfaction. The elegant and refined ambiance of the restaurant, along with the modern take on traditional Italian cuisine, must have made for a truly unforgettable dining experience. The dish inspired by the childhood memories of the chef, as well as the deconstructed lemon tart, show a high level of creativity and attention to detail. Overall, it sounds like a truly exceptional dining experience.\\n\\nFood lover 1: My response: I completely agree with your assessment. Your third meal sounds like it was a truly exceptional dining experience, with a high level of originality, presentation, and overall satisfaction. The elegant and refined ambiance of the restaurant, along with the modern take on traditional Italian cuisine, must have made for a truly unforgettable dining experience. The dish inspired by the childhood memories of the chef, as well as the deconstructed lemon tart, show a high level of creativity and attention to detail. I would rate your third meal very highly based on these factors. Next question.\\n\\nFood lover 2: Instruction: Based on our descriptions and ratings of each other\\'s meals, which meal would you be most interested in trying and why?\\nInput: Based on our descriptions and ratings of each other\\'s meals, I would be most interested in trying your second meal at Noma in Copenhagen, Denmark. The rustic and cozy ambiance of the restaurant, along with the creative and playful take on Nordic cuisine, sounds like a truly unique and memorable dining experience. The dessert made with fermented berries and ants also sounds like a fascinating and delicious dish to try. Overall, I would love to experience the beauty and flavors of Nordic cuisine at Noma.\\n\\nFood lover 1: My response: That\\'s a great choice! Based on our descriptions and ratings of each other\\'s meals, I would be most interested in trying your first meal at Osteria Francescana in Modena, Italy. The elegant and modern ambiance of the restaurant, along with the creative and artistic presentation of the dishes, sounds like a truly unforgettable dining experience. The \"Oops! I Dropped the Lemon Tart\" dessert also sounds like a playful and unexpected dish that would be a delight to try. Overall, I would love to experience the creativity and passion of Italian cuisine at Osteria Francescana.\\n\\nFood lover 2: Instruction: It seems like we both have a great choice in mind. Thank you for sharing your unforgettable meals with me. <TASK_DONE>\\n\\nFood lover 1: My response: You\\'re welcome! It was a pleasure sharing our unforgettable meals with each other. I look forward to collaborating with you on future tasks. <TASK_DONE>\\n\\nFood lover 2: Great! Looking forward to working with you again. <TASK_DONE>\\n\\nFood lover 1: Likewise! Have a great day. <TASK_DONE>\\n\\nFood lover 2: You too! <TASK_DONE>\\n\\nFood lover 1: Thank you! <TASK_DONE>\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the text into docs"
      ],
      "metadata": {
        "id": "FPcmGiUQiVdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Document(page_content=conversation)"
      ],
      "metadata": {
        "id": "4XKVN0TOHJo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs = RecursiveCharacterTextSplitter().split_documents([doc])"
      ],
      "metadata": {
        "id": "A0c_8-qqHAXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract Restaurant Info\n"
      ],
      "metadata": {
        "id": "OubDxc9VCh6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "     model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        ")"
      ],
      "metadata": {
        "id": "S9RZ9RZHCrPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Restaurant(BaseModel):\n",
        "    name: str = Field(\n",
        "        description=\"The name of the restaurant\",\n",
        "    )\n",
        "    location: Optional[str] = Field(\n",
        "        description=\"City and or country, the place where the restaurant is\",\n",
        "    )\n",
        "    style: Optional[str] = Field(\n",
        "        description=\"The types of cusine that is served at the restaurant\",\n",
        "    )\n",
        "    top_dish: Optional[str] = Field(\n",
        "        description=\"The top dish that people love the most\",\n",
        "    )\n",
        "\n",
        "    @validator(\"name\")\n",
        "    def name_must_not_be_empty(cls, v):\n",
        "        if not v:\n",
        "            raise ValueError(\"Name must not be empty\")\n",
        "        return v\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6_Wof-S4EKMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema, extraction_validator = from_pydantic(\n",
        "    Restaurant,\n",
        "    description=\"Extract information about restaurants including their name, location, style and dishes.\",\n",
        "    examples=[\n",
        "        (\n",
        "            \"My first fav meal was at a restaurant called Burnt Ends in Singapore.\",\n",
        "            {\"name\": \"Burnt Ends\", \"location\": \"Singapore\"},\n",
        "        )\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "wKJ6J7Ze0xhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(\n",
        "    llm,\n",
        "    schema,\n",
        "    encoder_or_encoder_class=\"csv\",\n",
        "    validator=extraction_validator,\n",
        "    input_formatter=\"triple_quotes\",\n",
        ")"
      ],
      "metadata": {
        "id": "TCYXBYvSFYEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57McFWe6uTnu",
        "outputId": "c5c6d7c2-509f-4ee2-8753-2d6217f82d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your goal is to extract structured information from the user's input that matches the form described below. When extracting information please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
            "\n",
            "```TypeScript\n",
            "\n",
            "restaurant: Array<{ // Extract information about restaurants including their name, location, style and dishes.\n",
            " name: string // The name of the restaurant\n",
            " location: string // City and or country, the place where the restaurant is\n",
            " style: string // The types of cusine that is served at the restaurant\n",
            " top_dish: string // The top dish that people love the most\n",
            "}>\n",
            "```\n",
            "\n",
            "\n",
            "Please output the extracted information in CSV format in Excel dialect. Please use a | as the delimiter. \n",
            " Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.\n",
            "\n",
            "\n",
            "\n",
            "Input: \"\"\"\n",
            "My first fav meal was at a restaurant called Burnt Ends in Singapore.\n",
            "\"\"\"\n",
            "Output: name|location|style|top_dish\n",
            "Burnt Ends|Singapore||\n",
            "\n",
            "Input: \"\"\"\n",
            "[user input]\n",
            "\"\"\"\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    document_extraction_results = await extract_from_documents(\n",
        "        chain, split_docs, max_concurrency=5, use_uid=False, return_exceptions=True\n",
        "    )\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Successful Requests: {cb.successful_requests}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLdZLuJ1GlgO",
        "outputId": "a6281a2e-2920-4f02-f7fa-e55b00a444c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 2745\n",
            "Prompt Tokens: 2583\n",
            "Completion Tokens: 162\n",
            "Successful Requests: 3\n",
            "Total Cost (USD): $0.00549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_extraction_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCAH5bPUGlj6",
        "outputId": "5a1fd306-4891-412a-83d0-14b2d996ca67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'uid': '0',\n",
              "  'source_uid': '0',\n",
              "  'data': {'restaurant': [{'name': 'El Celler de Can Roca',\n",
              "     'location': 'Girona, Spain',\n",
              "     'style': '',\n",
              "     'top_dish': 'forest-inspired dish'},\n",
              "    {'name': 'Noma',\n",
              "     'location': 'Copenhagen, Denmark',\n",
              "     'style': 'Nordic cuisine',\n",
              "     'top_dish': 'fermented berries and ants dessert'},\n",
              "    {'name': 'La Cava del Tequila',\n",
              "     'location': 'Mexico City, Mexico',\n",
              "     'style': 'Mexican',\n",
              "     'top_dish': 'mole'},\n",
              "    {'name': 'Gaggan',\n",
              "     'location': 'Bangkok, Thailand',\n",
              "     'style': 'modern Indian cuisine',\n",
              "     'top_dish': 'Lick It Up course'},\n",
              "    {'name': 'Osteria Francescana',\n",
              "     'location': 'Modena, Italy',\n",
              "     'style': 'modern Italian cuisine',\n",
              "     'top_dish': 'Oops! I Dropped the Lemon Tart'}]},\n",
              "  'raw': 'name|location|style|top_dish\\nEl Celler de Can Roca|Girona, Spain||forest-inspired dish\\nNoma|Copenhagen, Denmark|Nordic cuisine|fermented berries and ants dessert\\nLa Cava del Tequila|Mexico City, Mexico|Mexican|mole\\nGaggan|Bangkok, Thailand|modern Indian cuisine|\"Lick It Up\" course\\nOsteria Francescana|Modena, Italy|modern Italian cuisine|\"Oops! I Dropped the Lemon Tart\"',\n",
              "  'validated_data': [Restaurant(name='El Celler de Can Roca', location='Girona, Spain', style='', top_dish='forest-inspired dish'),\n",
              "   Restaurant(name='Noma', location='Copenhagen, Denmark', style='Nordic cuisine', top_dish='fermented berries and ants dessert'),\n",
              "   Restaurant(name='La Cava del Tequila', location='Mexico City, Mexico', style='Mexican', top_dish='mole'),\n",
              "   Restaurant(name='Gaggan', location='Bangkok, Thailand', style='modern Indian cuisine', top_dish='Lick It Up course'),\n",
              "   Restaurant(name='Osteria Francescana', location='Modena, Italy', style='modern Italian cuisine', top_dish='Oops! I Dropped the Lemon Tart')],\n",
              "  'errors': []},\n",
              " {'uid': '1',\n",
              "  'source_uid': '1',\n",
              "  'data': {'restaurant': [{'name': 'Attica',\n",
              "     'location': 'Melbourne, Australia',\n",
              "     'style': 'Australian cuisine',\n",
              "     'top_dish': 'Potato cooked in the earth it was grown'}]},\n",
              "  'raw': 'name|location|style|top_dish\\nAttica|Melbourne, Australia|Australian cuisine|Potato cooked in the earth it was grown',\n",
              "  'validated_data': [Restaurant(name='Attica', location='Melbourne, Australia', style='Australian cuisine', top_dish='Potato cooked in the earth it was grown')],\n",
              "  'errors': []},\n",
              " {'uid': '2',\n",
              "  'source_uid': '2',\n",
              "  'data': {'restaurant': []},\n",
              "  'raw': 'No structured information about restaurants is provided in this conversation. Therefore, I cannot extract any information about restaurants.',\n",
              "  'validated_data': [],\n",
              "  'errors': []}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's put it in a human readable format"
      ],
      "metadata": {
        "id": "8cc9_YenLuUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_restaurant_info(json_data):\n",
        "    for record in json_data:\n",
        "        restaurant_list = record.get('data', {}).get('restaurant', [])\n",
        "        for restaurant in restaurant_list:\n",
        "            name = restaurant.get('name', '')\n",
        "            location = restaurant.get('location', '')\n",
        "            style = restaurant.get('style', '')\n",
        "            top_dish = restaurant.get('top_dish', '')\n",
        "\n",
        "            # If style is not specified, we'll just say \"Cuisine not specified\"\n",
        "            style = style if style else 'Cuisine not specified'\n",
        "\n",
        "            print(f'Restaurant Name: {name}\\nLocation: {location}\\nStyle: {style}\\nTop Dish: {top_dish}\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "IGAiZqjvGlm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "extract_restaurant_info(document_extraction_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MUbvaicJL8W",
        "outputId": "7230af85-6eb8-40cd-8cd2-033817db1cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restaurant Name: El Celler de Can Roca\n",
            "Location: Girona, Spain\n",
            "Style: Cuisine not specified\n",
            "Top Dish: forest-inspired dish\n",
            "\n",
            "Restaurant Name: Noma\n",
            "Location: Copenhagen, Denmark\n",
            "Style: Nordic cuisine\n",
            "Top Dish: fermented berries and ants dessert\n",
            "\n",
            "Restaurant Name: La Cava del Tequila\n",
            "Location: Mexico City, Mexico\n",
            "Style: Mexican\n",
            "Top Dish: mole\n",
            "\n",
            "Restaurant Name: Gaggan\n",
            "Location: Bangkok, Thailand\n",
            "Style: modern Indian cuisine\n",
            "Top Dish: Lick It Up course\n",
            "\n",
            "Restaurant Name: Osteria Francescana\n",
            "Location: Modena, Italy\n",
            "Style: modern Italian cuisine\n",
            "Top Dish: Oops! I Dropped the Lemon Tart\n",
            "\n",
            "Restaurant Name: Attica\n",
            "Location: Melbourne, Australia\n",
            "Style: Australian cuisine\n",
            "Top Dish: Potato cooked in the earth it was grown\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets put it in a structured DataFrame"
      ],
      "metadata": {
        "id": "UHWoSa6NLpXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_dataframe(json_data):\n",
        "    # Prepare an empty list to store all restaurant data\n",
        "    data = []\n",
        "\n",
        "    for record in json_data:\n",
        "        restaurant_list = record.get('data', {}).get('restaurant', [])\n",
        "        for restaurant in restaurant_list:\n",
        "            # Get details for each restaurant and append it to data\n",
        "            data.append([\n",
        "                restaurant.get('name', ''),\n",
        "                restaurant.get('location', ''),\n",
        "                restaurant.get('style', '') if restaurant.get('style', '') else 'Cuisine not specified',\n",
        "                restaurant.get('top_dish', '')\n",
        "            ])\n",
        "\n",
        "    # Convert the list into a DataFrame\n",
        "    df = pd.DataFrame(data, columns=['Name', 'Location', 'Style', 'Top Dish'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage:\n",
        "df = generate_dataframe(document_extraction_results)\n"
      ],
      "metadata": {
        "id": "Xel0E62FJL-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "aul5vTtbJMDr",
        "outputId": "1d54a7f0-5437-4c7e-85bb-e53ae13263dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Name              Location                   Style  \\\n",
              "0  El Celler de Can Roca         Girona, Spain   Cuisine not specified   \n",
              "1                   Noma   Copenhagen, Denmark          Nordic cuisine   \n",
              "2    La Cava del Tequila   Mexico City, Mexico                 Mexican   \n",
              "3                 Gaggan     Bangkok, Thailand   modern Indian cuisine   \n",
              "4    Osteria Francescana         Modena, Italy  modern Italian cuisine   \n",
              "5                 Attica  Melbourne, Australia      Australian cuisine   \n",
              "\n",
              "                                  Top Dish  \n",
              "0                     forest-inspired dish  \n",
              "1       fermented berries and ants dessert  \n",
              "2                                     mole  \n",
              "3                        Lick It Up course  \n",
              "4           Oops! I Dropped the Lemon Tart  \n",
              "5  Potato cooked in the earth it was grown  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-321e5ede-152a-486b-936f-2ccd34d158da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Location</th>\n",
              "      <th>Style</th>\n",
              "      <th>Top Dish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>El Celler de Can Roca</td>\n",
              "      <td>Girona, Spain</td>\n",
              "      <td>Cuisine not specified</td>\n",
              "      <td>forest-inspired dish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Noma</td>\n",
              "      <td>Copenhagen, Denmark</td>\n",
              "      <td>Nordic cuisine</td>\n",
              "      <td>fermented berries and ants dessert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>La Cava del Tequila</td>\n",
              "      <td>Mexico City, Mexico</td>\n",
              "      <td>Mexican</td>\n",
              "      <td>mole</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gaggan</td>\n",
              "      <td>Bangkok, Thailand</td>\n",
              "      <td>modern Indian cuisine</td>\n",
              "      <td>Lick It Up course</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Osteria Francescana</td>\n",
              "      <td>Modena, Italy</td>\n",
              "      <td>modern Italian cuisine</td>\n",
              "      <td>Oops! I Dropped the Lemon Tart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Attica</td>\n",
              "      <td>Melbourne, Australia</td>\n",
              "      <td>Australian cuisine</td>\n",
              "      <td>Potato cooked in the earth it was grown</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-321e5ede-152a-486b-936f-2ccd34d158da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-321e5ede-152a-486b-936f-2ccd34d158da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-321e5ede-152a-486b-936f-2ccd34d158da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXMNJquwJMHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema, validator = from_pydantic(Restaurant)"
      ],
      "metadata": {
        "id": "cSwBpVV2JMJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(\n",
        "    llm,\n",
        "    schema,\n",
        "    encoder_or_encoder_class=\"csv\",\n",
        "    validator=validator,\n",
        "    input_formatter=\"triple_quotes\",\n",
        ")"
      ],
      "metadata": {
        "id": "jOqsDAq34hne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    document_extraction_results = await extract_from_documents(\n",
        "        chain, split_docs, max_concurrency=5, use_uid=False, return_exceptions=True\n",
        "    )\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Successful Requests: {cb.successful_requests}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6c1bfc-ed5f-4d3c-cb00-8a3a540617c2",
        "id": "a1Ch17pW4hnq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 2666\n",
            "Prompt Tokens: 2412\n",
            "Completion Tokens: 254\n",
            "Successful Requests: 3\n",
            "Total Cost (USD): $0.005332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_extraction_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34925efd-0c81-4845-d591-64cb52c2242b",
        "id": "oHeuZQAj4hnq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'uid': '0',\n",
              "  'source_uid': '0',\n",
              "  'data': {'restaurant': [{'name': '-',\n",
              "     'location': '-',\n",
              "     'style': '-',\n",
              "     'top_dish': '-'},\n",
              "    {'name': 'El Celler de Can Roca',\n",
              "     'location': 'Girona, Spain',\n",
              "     'style': 'Creative and delicious',\n",
              "     'top_dish': 'Forest-inspired dish'},\n",
              "    {'name': 'Noma',\n",
              "     'location': 'Copenhagen, Denmark',\n",
              "     'style': 'Simple and natural Nordic cuisine',\n",
              "     'top_dish': 'Dessert made with fermented berries and ants'},\n",
              "    {'name': 'La Cava del Tequila',\n",
              "     'location': 'Mexico City',\n",
              "     'style': 'Authentic and flavorful regional specialties',\n",
              "     'top_dish': 'Mole'},\n",
              "    {'name': 'Gaggan',\n",
              "     'location': 'Bangkok, Thailand',\n",
              "     'style': 'Creative and playful Indian cuisine',\n",
              "     'top_dish': 'Lick It Up course'},\n",
              "    {'name': 'Osteria Francescana',\n",
              "     'location': 'Modena, Italy',\n",
              "     'style': 'Modern take on traditional Italian cuisine',\n",
              "     'top_dish': 'Oops! I Dropped the Lemon Tart'}]},\n",
              "  'raw': 'name|location|style|top_dish\\n-|-|-|-\\nEl Celler de Can Roca|Girona, Spain|Creative and delicious|Forest-inspired dish\\nNoma|Copenhagen, Denmark|Simple and natural Nordic cuisine|Dessert made with fermented berries and ants\\nLa Cava del Tequila|Mexico City|Authentic and flavorful regional specialties|Mole\\nGaggan|Bangkok, Thailand|Creative and playful Indian cuisine|\"Lick It Up\" course\\nOsteria Francescana|Modena, Italy|Modern take on traditional Italian cuisine|\"Oops! I Dropped the Lemon Tart\"',\n",
              "  'validated_data': None,\n",
              "  'errors': [ValidationError(model='Restaurant', errors=[{'loc': ('__root__',), 'msg': 'Restaurant expected dict not list', 'type': 'type_error'}])]},\n",
              " {'uid': '1',\n",
              "  'source_uid': '1',\n",
              "  'data': {'restaurant': [{'restaurant_name': '---',\n",
              "     'location': '---',\n",
              "     'style': '---',\n",
              "     'top_dish': '---'},\n",
              "    {'restaurant_name': 'Attica',\n",
              "     'location': 'Melbourne, Australia',\n",
              "     'style': 'Australian',\n",
              "     'top_dish': 'Potato cooked in the earth it was grown'},\n",
              "    {'restaurant_name': 'N/A',\n",
              "     'location': 'N/A',\n",
              "     'style': 'Mexican',\n",
              "     'top_dish': 'N/A'},\n",
              "    {'restaurant_name': 'N/A',\n",
              "     'location': 'N/A',\n",
              "     'style': 'Italian',\n",
              "     'top_dish': 'deconstructed lemon tart'}]},\n",
              "  'raw': 'restaurant_name|location|style|top_dish\\n---|---|---|---\\nAttica|Melbourne, Australia|Australian|Potato cooked in the earth it was grown\\nN/A|N/A|Mexican|N/A\\nN/A|N/A|Italian|deconstructed lemon tart',\n",
              "  'validated_data': None,\n",
              "  'errors': [ValidationError(model='Restaurant', errors=[{'loc': ('__root__',), 'msg': 'Restaurant expected dict not list', 'type': 'type_error'}])]},\n",
              " {'uid': '2',\n",
              "  'source_uid': '2',\n",
              "  'data': {'restaurant': [{'restaurant_name': '---',\n",
              "     'location': '---',\n",
              "     'style': '---',\n",
              "     'top_dish': '---'},\n",
              "    {'restaurant_name': 'Osteria Francescana',\n",
              "     'location': 'Modena, Italy',\n",
              "     'style': 'Italian',\n",
              "     'top_dish': 'Oops! I Dropped the Lemon Tart'},\n",
              "    {'restaurant_name': 'Noma',\n",
              "     'location': 'Copenhagen, Denmark',\n",
              "     'style': 'Nordic',\n",
              "     'top_dish': 'Dessert made with fermented berries and ants'}]},\n",
              "  'raw': 'restaurant_name|location|style|top_dish\\n---|---|---|---\\nOsteria Francescana|Modena, Italy|Italian|Oops! I Dropped the Lemon Tart\\nNoma|Copenhagen, Denmark|Nordic|Dessert made with fermented berries and ants',\n",
              "  'validated_data': None,\n",
              "  'errors': [ValidationError(model='Restaurant', errors=[{'loc': ('__root__',), 'msg': 'Restaurant expected dict not list', 'type': 'type_error'}])]}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "extract_restaurant_info(document_extraction_results)"
      ],
      "metadata": {
        "id": "F6ayES-fJMMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6957e71e-e479-432a-bca8-c1906c4c2d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restaurant Name: -\n",
            "Location: -\n",
            "Style: -\n",
            "Top Dish: -\n",
            "\n",
            "Restaurant Name: El Celler de Can Roca\n",
            "Location: Girona, Spain\n",
            "Style: Creative and delicious\n",
            "Top Dish: Forest-inspired dish\n",
            "\n",
            "Restaurant Name: Noma\n",
            "Location: Copenhagen, Denmark\n",
            "Style: Simple and natural Nordic cuisine\n",
            "Top Dish: Dessert made with fermented berries and ants\n",
            "\n",
            "Restaurant Name: La Cava del Tequila\n",
            "Location: Mexico City\n",
            "Style: Authentic and flavorful regional specialties\n",
            "Top Dish: Mole\n",
            "\n",
            "Restaurant Name: Gaggan\n",
            "Location: Bangkok, Thailand\n",
            "Style: Creative and playful Indian cuisine\n",
            "Top Dish: Lick It Up course\n",
            "\n",
            "Restaurant Name: Osteria Francescana\n",
            "Location: Modena, Italy\n",
            "Style: Modern take on traditional Italian cuisine\n",
            "Top Dish: Oops! I Dropped the Lemon Tart\n",
            "\n",
            "Restaurant Name: \n",
            "Location: ---\n",
            "Style: ---\n",
            "Top Dish: ---\n",
            "\n",
            "Restaurant Name: \n",
            "Location: Melbourne, Australia\n",
            "Style: Australian\n",
            "Top Dish: Potato cooked in the earth it was grown\n",
            "\n",
            "Restaurant Name: \n",
            "Location: N/A\n",
            "Style: Mexican\n",
            "Top Dish: N/A\n",
            "\n",
            "Restaurant Name: \n",
            "Location: N/A\n",
            "Style: Italian\n",
            "Top Dish: deconstructed lemon tart\n",
            "\n",
            "Restaurant Name: \n",
            "Location: ---\n",
            "Style: ---\n",
            "Top Dish: ---\n",
            "\n",
            "Restaurant Name: \n",
            "Location: Modena, Italy\n",
            "Style: Italian\n",
            "Top Dish: Oops! I Dropped the Lemon Tart\n",
            "\n",
            "Restaurant Name: \n",
            "Location: Copenhagen, Denmark\n",
            "Style: Nordic\n",
            "Top Dish: Dessert made with fermented berries and ants\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtpxoHWqrhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VeyDe3YYrh2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmRdOHM1rh5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTBeKTe6rh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtqjVbnyrh_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_RqAoNxriB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zY6NmWBari-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDAgdes_rjBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W57jeVugrjEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPW-Gfm6rjGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = create_extraction_chain(llm, restaurant_schema)"
      ],
      "metadata": {
        "id": "6oplQu-FDHks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "891192db-56cd-4865-b3e9-fa8b73f27a67",
        "id": "TmhWuWPSDHkt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Food lover 2: Instruction: Please describe your first most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My first most unforgettable meal was at a restaurant called El Celler de Can Roca in Girona, Spain. The ambiance was elegant and modern, and the food was a creative and delicious 18-course tasting menu. One unique experience was when they brought out a dish that was inspired by the smells of the forest.\\n\\nFood lover 1: My response: That sounds amazing! The forest-inspired dish must have been a unique experience. My first most unforgettable meal was at a restaurant called Noma in Copenhagen, Denmark. The location was in an old warehouse by the waterfront, and the ambiance was rustic and cozy. The food was presented in a simple and natural way, with many of the ingredients sourced from the surrounding Nordic region. One of the most memorable dishes was a dessert made with fermented berries and ants, which added a surprising and delicious crunch. Overall, it was a truly unforgettable dining experience that showcased the beauty and flavors of Nordic cuisine. Next question.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = sections[0]\n",
        "\n",
        "extracted = extraction_chain.predict_and_parse(text=(text))[\"data\"]\n",
        "\n",
        "print(extracted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b83ca1-8d0c-4265-8c03-9c5784032ee9",
        "id": "oDJcwe0MDHkt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'restaurant': [{'name': 'El Celler de Can Roca'}, {'name': 'location'}, {'name': 'Girona, Spain'}, {'name': 'ambiance'}, {'name': 'elegant and modern'}, {'name': 'taste'}, {'name': 'creative and delicious 18-course tasting menu'}, {'name': 'unique experience'}, {'name': 'forest-inspired dish'}, {'name': 'name'}, {'name': 'Noma'}, {'name': 'location'}, {'name': 'Copenhagen, Denmark'}, {'name': 'ambiance'}, {'name': 'rustic and cozy'}, {'name': 'taste'}, {'name': 'simple and natural'}, {'name': 'unique experience'}, {'name': 'dessert made with fermented berries and ants'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xofp61TBghC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5-4Q5vWBgkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGf3Y_KPBgm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJioaZxYBgqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_conversation(filename, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Load a conversation from a file and split it into sections of approximately 2048 tokens.\n",
        "\n",
        "    Parameters:\n",
        "    filename (str): The name of the file to read the conversation from.\n",
        "    max_tokens (int): The maximum number of tokens per section.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of strings, where each string is a section of the conversation.\n",
        "    \"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        conversation = f.read()\n",
        "\n",
        "    # Split the conversation into turns\n",
        "    turns = conversation.split(\"\\n\\n\")\n",
        "\n",
        "    sections = []\n",
        "    section = \"\"\n",
        "\n",
        "    for turn in turns:\n",
        "        # If adding the next turn would exceed the maximum number of tokens,\n",
        "        # add the current section to the list and start a new section\n",
        "        if len(section.split()) + len(turn.split()) > max_tokens:\n",
        "            sections.append(section.strip())\n",
        "            section = \"\"\n",
        "\n",
        "        # Add the turn to the current section\n",
        "        section += f\"{turn}\\n\\n\"\n",
        "\n",
        "    # Add the last section to the list\n",
        "    sections.append(section.strip())\n",
        "\n",
        "    return sections\n",
        "\n"
      ],
      "metadata": {
        "id": "42xbcr_zpr1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-H0Zo6ElBgsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zy9oZpCgBgvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_schema = Object(\n",
        "    id=\"restaurant\",\n",
        "    description=(\n",
        "        \"People are talking about restaurants names and dishes as well as qualities of the restaturant\"\n",
        "    ),\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"name\",\n",
        "            description=\"The name of the restaurant\"\n",
        "        )\n",
        "    ],\n",
        "    examples=[(\"We went for a quick bite at McDonalds\",[{\"name\": \"McDonalds\"}]),\n",
        "            (\"I just love the steaks at Mortons\",[{\"name\": \"Mortons\"}]),\n",
        "            (\"We already have a booking at The Eatery so can't goto Mortons\",[{\"name\": \"The Eatery\"},{\"name\": \"Mortons\"}])\n",
        "            ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "LPNKNO_7CrPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5XOgiPoBgxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mgq6rHZmBg1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJRr4rQFBg3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdrBK20cBg5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### with browsing\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kor!\n",
        "from kor.extraction import create_extraction_chain\n",
        "from kor.nodes import Object, Text, Number\n",
        "\n",
        "# LangChain Models\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Standard Helpers\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Text Helpers\n",
        "from bs4 import BeautifulSoup\n",
        "from markdownify import markdownify as md\n",
        "\n",
        "# For token counting\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def printOutput(output):\n",
        "    print(json.dumps(output,sort_keys=True, indent=3))"
      ],
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the text file"
      ],
      "metadata": {
        "id": "GIe9SKlwpsup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the model"
      ],
      "metadata": {
        "id": "-4r3I0TBuJ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "     model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    # max_tokens=2048,\n",
        ")"
      ],
      "metadata": {
        "id": "IfCt8bhHNu9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_schema = Object(\n",
        "    id=\"restaurant\",\n",
        "    description=(\n",
        "        \"People are talking about restaurants and dishes as well as qualities of the restaturant\"\n",
        "    ),\n",
        "    attributes=[\n",
        "        Text(\n",
        "            id=\"name\",\n",
        "            description=\"The name of the restaurant\"\n",
        "        )\n",
        "    ],\n",
        "    examples=[(\"We went for a quick bite at McDonalds\",[{\"name\": \"McDonalds\"}]),\n",
        "                      (\"I just love the steaks at Mortons\",[{\"name\": \"Mortons\"}]),\n",
        "                      (\"We already have a booking at The Eatery so can't goto Mortons\",[{\"name\": \"The Eatery\"},{\"name\": \"Mortons\"}])\n",
        "                      ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "G-O1nZ2Jpfec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restaurant_schema = Object(\n",
        "\n",
        "#     id=\"restaurant\",\n",
        "\n",
        "#     # Natural language description about your object\n",
        "#     description=\"Personal information about a person\",\n",
        "\n",
        "#     # Fields you'd like to capture from a piece of text about your object.\n",
        "#     attributes=[\n",
        "#         Text(\n",
        "#             id=\"first_name\",\n",
        "#             description=\"The first name of a person.\",\n",
        "#         )\n",
        "#     ],\n",
        "\n",
        "#     # Examples help go a long way with telling the LLM what you need\n",
        "#     examples=[\n",
        "#         (\"Alice and Bob are friends\", [{\"first_name\": \"Alice\"}, {\"first_name\": \"Bob\"}])\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "Ex0SPwExa6s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_extraction_chain(llm, restaurant_schema)"
      ],
      "metadata": {
        "id": "J2BbC0Q6cSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "tG8gw5DWyRgj",
        "outputId": "93f878e0-edc6-4c9f-ab92-09741d89645e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Food lover 2: Instruction: Please describe your first most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My first most unforgettable meal was at a restaurant called El Celler de Can Roca in Girona, Spain. The ambiance was elegant and modern, and the food was a creative and delicious 18-course tasting menu. One unique experience was when they brought out a dish that was inspired by the smells of the forest.\\n\\nFood lover 1: My response: That sounds amazing! The forest-inspired dish must have been a unique experience. My first most unforgettable meal was at a restaurant called Noma in Copenhagen, Denmark. The location was in an old warehouse by the waterfront, and the ambiance was rustic and cozy. The food was presented in a simple and natural way, with many of the ingredients sourced from the surrounding Nordic region. One of the most memorable dishes was a dessert made with fermented berries and ants, which added a surprising and delicious crunch. Overall, it was a truly unforgettable dining experience that showcased the beauty and flavors of Nordic cuisine. Next question.\\n\\nFood lover 2: Instruction: Please describe your second most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My second most unforgettable meal was at a small family-owned restaurant called La Cava del Tequila in Mexico City. The ambiance was cozy and intimate, with dim lighting and traditional Mexican decor. The food was authentic and flavorful, with a focus on regional specialties. One unique experience was when the owner came to our table and gave us a tequila tasting, explaining the different types and flavors. The standout dish for me was the mole, which had a complex and rich flavor that I had never tasted before.\\n\\nFood lover 1: My response: That sounds like a wonderful experience! My second most unforgettable meal was at a restaurant called Gaggan in Bangkok, Thailand. The location was in a beautiful colonial-style house, and the ambiance was modern and chic. The food was a creative and playful take on Indian cuisine, with each dish presented in a unique and artistic way. One unique experience was when they brought out a dish that was meant to be eaten with your hands, and it was served on a miniature replica of a tree. The standout dish for me was the \"Lick It Up\" course, which was a deconstructed version of a traditional Indian street food dish. Overall, it was an unforgettable dining experience that showcased the creativity and innovation of modern Indian cuisine. Next question.\\n\\nFood lover 2: Instruction: Please describe your third most unforgettable meal, including the location, ambiance, taste, and any unique experiences.\\nInput: My third most unforgettable meal was at a restaurant called Osteria Francescana in Modena, Italy. The location was in a historic building in the heart of the city, and the ambiance was elegant and refined. The food was a modern take on traditional Italian cuisine, with a focus on local and seasonal ingredients. One unique experience was when they brought out a dish that was inspired by the childhood memories of the chef, Massimo Bottura. The standout dish for me was the \"Oops! I Dropped the Lemon Tart,\" which was a deconstructed version of a lemon tart that was presented in a playful and unexpected way. Overall, it was an unforgettable dining experience that showcased the creativity and passion of Italian cuisine.\\n\\nFood lover 1: My response: That\\'s a great choice! My third most unforgettable meal was at a restaurant called Attica in Melbourne, Australia. The location was in a converted warehouse, and the ambiance was modern and minimalist. The food was a creative and innovative take on Australian cuisine, with a focus on native ingredients and flavors. One unique experience was when they brought out a dish that was meant to be eaten with your hands, and it was served on a bed of eucalyptus leaves. The standout dish for me was the \"Potato cooked in the earth it was grown,\" which was a simple yet delicious dish that showcased the natural flavors of the potato. Overall, it was an unforgettable dining experience that showcased the beauty and diversity of Australian cuisine. Next question.\\n\\nFood lover 2: Instruction: Please rate my first meal based on originality, presentation, and overall satisfaction.\\nInput: I would rate your first meal very highly based on originality, presentation, and overall satisfaction. The forest-inspired dish sounds like a unique and creative concept, and the elegant and modern ambiance of the restaurant must have added to the overall experience. The 18-course tasting menu also shows a high level of creativity and attention to detail. Overall, it sounds like a truly unforgettable dining experience.\\n\\nFood lover 1: My response: I completely agree with your assessment. Your first meal sounds like it was a truly unforgettable dining experience, with a high level of originality, presentation, and overall satisfaction. The attention to detail and creativity in the dishes, as well as the elegant and modern ambiance of the restaurant, must have made for a truly memorable experience. I would rate your first meal very highly based on these factors. Next question.\\n\\nFood lover 2: Instruction: Please rate my second meal based on originality, presentation, and overall satisfaction.\\nInput: I would rate your second meal very highly based on originality, presentation, and overall satisfaction. The cozy and intimate ambiance of the restaurant, along with the authentic and flavorful Mexican cuisine, must have made for a truly memorable experience. The tequila tasting and focus on regional specialties also show a high level of attention to detail and passion for the cuisine. Overall, it sounds like a wonderful dining experience.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = sections[0]\n",
        "output = chain.predict_and_parse(text=(text))[\"data\"]\n",
        "\n",
        "printOutput(output)"
      ],
      "metadata": {
        "id": "3BjtPl9Va8fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5314b80e-d21c-4542-f2c6-6e8665e1d79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "   \"restaurant\": [\n",
            "      {\n",
            "         \"name\": \"La Cava del Tequila\"\n",
            "      }\n",
            "   ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = chain.predict_and_parse(text=(\"The dog went to the park\"))[\"data\"]\n",
        "printOutput(output)"
      ],
      "metadata": {
        "id": "VNai9v6Ocx_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a4e3b7-acb4-4f9b-f2d8-3c9251ff403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "   \"person\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Fields"
      ],
      "metadata": {
        "id": "z9KCWGEypyNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " (\"I had the fresh pasta with cream\", \"fresh pasta with cream\"),\n",
        "        #                 (\"for me the steak frites was a good choice on my diet\",\"steak frites\"),\n",
        "        #                 (\"The grilled octopus was so yummy\",\"grilled octopus\"),\n",
        "        #                 (\"I had to send the fish tacos back as they were raw\",\"fish tacos\"),\n",
        "        #             ],\n",
        "        #     many=True,\n",
        "        # ),\n",
        "    ],\n",
        "    many=True,\n",
        ")"
      ],
      "metadata": {
        "id": "K2rNiJa-8nIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    result = chain.predict_and_parse(text=text)\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Successful Requests: {cb.successful_requests}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pfOz3irt9Kd",
        "outputId": "07f05d70-b6b4-4239-dbb5-0a3a86d78af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 1858\n",
            "Prompt Tokens: 1847\n",
            "Completion Tokens: 11\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0037159999999999997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_L8AHdzuDhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}